{
    "html": "<!DOCTYPE html><html lang=\"zh-cn\" style=\"font-size: 67px;\"><head>\n<title>弋力-交叉信息研究院</title><meta name=\"keywords\" content=\"交叉信息研究院,弋力\">\n\n<meta charset=\"utf-8\">\n<meta name=\"renderer\" content=\"webkit\">\n\n<meta name=\"description\" content=\"\">\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1\">\n<meta name=\"format-detection\" content=\"telephone=no,email=no,adress=no,date=no\">\n<link rel=\"apple-touch-icon-precomposed\" href=\"../../images/favicon.png\">\n<link rel=\"icon\" sizes=\"72*72\" href=\"../../images/favicon.png\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"../../css/public.css\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"../../css/body.css\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"../../css/ny.css\">\n<script type=\"text/javascript\" src=\"../../js/jq.js\"></script>\n<!--[if lte IE 9]>\n  <script type=\"text/javascript\" src=\"../../js/jquery.js\"></script>\n<![endif]-->\n<script src=\"../../js/script.js\"></script>\n<!--Announced by Visual SiteBuilder 9-->\n<link rel=\"stylesheet\" type=\"text/css\" href=\"../../_sitegray/_sitegray_d.css\">\n<script language=\"javascript\" src=\"../../_sitegray/_sitegray.js\"></script>\n<!-- CustomerNO:77656262657232307e78475c5356574200000003435c -->\n<link rel=\"stylesheet\" type=\"text/css\" href=\"../../rydw-dp-xgyjgz.vsb.css\">\n<script type=\"text/javascript\" src=\"/system/resource/js/counter.js\"></script>\n<script type=\"text/javascript\">_jsq_(1075,'/rydw-dp-xgyjgz.jsp',-1,2110873241)</script>\n</head>\n<body data-aos-easing=\"ease\" data-aos-duration=\"1200\" data-aos-delay=\"0\">\n<div id=\"load\" style=\"display: none;\"></div>\n\n<div id=\"app\">\n    \n    <header class=\"header\">\n        <div class=\"wp flexjs\">\n          <div class=\"logo\">\n            \n\n\n\n\n\n\n\n<script language=\"javascript\" src=\"/system/resource/js/dynclicks.js\"></script><script language=\"javascript\" src=\"/system/resource/js/centerCutImg.js\"></script><p>\n</p>\n\n            \n            <div class=\"a\">\n                  <a href=\"../../index.htm\" onclick=\"_addDynClicks(&quot;wbimage&quot;, 2110873241, 1019076)\"><img src=\"../../images/logo.png\" border=\"0\" class=\"i1\"></a>\n\n              \n<!-- 网站logo图片地址请在本组件\"内容配置-网站logo\"处填写 -->\n<a href=\"../../index.htm\" title=\"交叉信息研究院主页\"><img src=\"../../images/logos.png\" alt=\"\" class=\"i2\"></a>\n            </div>\n          </div>\n\n          <div class=\"topr flexjs flexc\">\n            <div class=\"toplink flex\">\n              <ul class=\"flex\">\n                <li>    \n<a href=\"javascript:;\" class=\"a\" onclick=\"_addDynClicks(&quot;wbimage&quot;, 2110873241, 1019079)\"><i class=\"swi-weixin0\"></i>微信公众号</a>\n                  <div class=\"ewm\">\n                    <img src=\"../../images/ewm.png\" border=\"0\">\n                  </div>\n                \n\n</li>\n                <script language=\"javascript\" src=\"/system/resource/js/openlink.js\"></script>\n<li>\n                  <a href=\"http://cqi.tsinghua.edu.cn/\" title=\"\" onclick=\"_addDynClicks(&quot;wburl&quot;, 2110873241, 108887)\" class=\"a\"><i class=\"swi-shouye\"></i>CQI官网</a>\n                </li>\n    \n\n\n                \n                <li><a href=\"https://iiis.tsinghua.edu.cn/en/People/Faculty/YiLi.htm\" class=\"a\">EN</a></li>\n                <li>\n                  <a href=\"javascript:;\" class=\"showSear swi-search-outlined\"></a>\n                </li>\n              </ul>\n            </div>\n            <div class=\"topnav flex\">\n              <ul class=\"flex\">   <li><a href=\"../../index.htm\">首页</a></li>\n   \n\n                <li>\n                  <a href=\"../../yxgk/yxjj.htm\">院系概况</a>\n                  <i class=\"plus swi-down-outlined\"></i>\n                  <div class=\"sub-nav\">\n                    <div class=\"sub-nav-h\">\n                      \n                      <a href=\"../../yxgk/yxjj.htm\">院系简介</a>\n                      \n                      <a href=\"../../yxgk/yzjy.htm\">院长寄语</a>\n                      \n                      <a href=\"../../yxgk/xrld.htm\">现任领导</a>\n                      \n                      <a href=\"../../yxgk/yxyg.htm\">院系沿革</a>\n                    </div>\n                  </div>\n                </li>\n\n                <li>\n                  <a href=\"../../xwdt/yxdt.htm\">新闻动态</a>\n                  <i class=\"plus swi-down-outlined\"></i>\n                  <div class=\"sub-nav\">\n                    <div class=\"sub-nav-h\">\n                      \n                      <a href=\"../../xwdt/yxdt.htm\">院系动态</a>\n                      \n                      <a href=\"../../xwdt/mtjj.htm\">媒体聚焦</a>\n                    </div>\n                  </div>\n                </li>\n\n                <li>\n                  <a href=\"../../rydw.htm\">人员队伍</a>\n                  <i class=\"plus swi-down-outlined\"></i>\n                  <div class=\"sub-nav\">\n                    <div class=\"sub-nav-h\">\n                      \n                      <a href=\"../../rydw.htm#sz1\" class=\"at\">全职教师</a>\n                      \n                      <a href=\"../../rydw.htm#sz8\">研究系列</a>\n                      \n                      <a href=\"../../rydw.htm#sz2\">荣誉客座</a>\n                      \n                      <a href=\"../../rydw.htm#sz3\">兼职教师</a>\n                      \n                      <a href=\"../../rydw.htm#sz4\">博士后</a>\n                      \n                      <a href=\"../../rydw.htm#sz5\">行政人员</a>\n                      \n                      <a href=\"../../rydw.htm#sz6\">实验员</a>\n                    </div>\n                  </div>\n                </li>\n\n                <li>\n                  <a href=\"../../ybbks/ybpy.htm\">姚班本科生</a>\n                  <i class=\"plus swi-down-outlined\"></i>\n                  <div class=\"sub-nav\">\n                    <div class=\"sub-nav-h\">\n                      \n                      <a href=\"../../ybbks/ybpy.htm\">姚班培养</a>\n                      \n                      <a href=\"../../ybbks/ybzs.htm\">姚班招生</a>\n                    </div>\n                  </div>\n                </li>\n\n                <li>\n                  <a href=\"../../yjs/yjspy.htm\">研究生</a>\n                  <i class=\"plus swi-down-outlined\"></i>\n                  <div class=\"sub-nav\">\n                    <div class=\"sub-nav-h\">\n                      \n                      <a href=\"../../yjs/yjspy.htm\">研究生培养</a>\n                      \n                      <a href=\"../../yjs/yjszs.htm\">研究生招生</a>\n                    </div>\n                  </div>\n                </li>\n\n                <li>\n                  <a href=\"../../lzzx/zxjj.htm\">量子中心</a>\n                  <i class=\"plus swi-down-outlined\"></i>\n                  <div class=\"sub-nav\">\n                    <div class=\"sub-nav-h\">\n                      \n                      <a href=\"../../lzzx/zxjj.htm\">中心简介</a>\n                      \n                      <a href=\"../../lzzx/zxjj.htm#lzsz\">师资力量</a>\n                      \n                      <a href=\"../../lzzx/kygk.htm\">科研概况</a>\n                      \n                      <a href=\"../../lzzx/lzzxdt.htm\">量子中心动态</a>\n                    </div>\n                  </div>\n                </li>\n\n                <li>\n                  <a href=\"../../kxyj/zzjg.htm\">科学研究</a>\n                  <i class=\"plus swi-down-outlined\"></i>\n                  <div class=\"sub-nav\">\n                    <div class=\"sub-nav-h\">\n                      \n                      <a href=\"../../kxyj/zzjg.htm\">组织架构</a>\n                      \n                      <a href=\"../../kxyj/ktzjs.htm\">课题组介绍</a>\n                      \n                      <a href=\"../../kxyj/yjdt.htm\">研究动态</a>\n                      \n                      <a href=\"../../kxyj/kyjb.htm\">科研简报</a>\n                    </div>\n                  </div>\n                </li>\n\n                <li>\n                  <a href=\"../../xysh/xshd/nljlb.htm\">校园生活</a>\n                  <i class=\"plus swi-down-outlined\"></i>\n                  <div class=\"sub-nav\">\n                    <div class=\"sub-nav-h\">\n                      \n                      <a href=\"../../xysh/xshd/nljlb.htm\">学生活动</a>\n                      \n                      <a href=\"../../xysh/yszx.htm\">衣食住行</a>\n                    </div>\n                  </div>\n                </li>\n\n                <li>\n                  <a href=\"../../gkzp.htm\">公开招聘</a>\n                  <i class=\"plus swi-down-outlined\"></i>\n                  <div class=\"sub-nav\">\n                    <div class=\"sub-nav-h\">\n                      \n                      <a href=\"../../gkzp/jszp.htm\">教师招聘</a>\n                      \n                      <a href=\"../../gkzp/bshzp.htm\">博士后招聘</a>\n                      \n                      <a href=\"../../gkzp/zyzp.htm\">职员招聘</a>\n                    </div>\n                  </div>\n                </li>\n\n                <li>\n                  <a href=\"../../xypd/xyfc.htm\">校友频道</a>\n                  <i class=\"plus swi-down-outlined\"></i>\n                  <div class=\"sub-nav\">\n                    <div class=\"sub-nav-h\">\n                      \n                      <a href=\"../../xypd/xyfc.htm\">校友风采</a>\n                      \n                      <a href=\"../../xypd/xyjlb/zqjlb.htm\">校友俱乐部</a>\n                      \n                      <a href=\"../../xypd/xyjz.htm\">校友捐赠</a>\n                    </div>\n                  </div>\n                </li>\n\n\n                \n                \n</ul>\n\n              <div id=\"openBtn\" class=\"btn rd-navbar-toggle\">\n                <div class=\"lcbody\">\n                  <div class=\"lcitem top\">\n                    <div class=\"rect top\"></div>\n                  </div>\n                  <div class=\"lcitem center hide\">\n                    <div class=\"rect bottom\"></div>\n                  </div>\n                  <div class=\"lcitem bottom\">\n                    <div class=\"rect bottom\"></div>\n                  </div>\n                </div>\n              </div>\n            </div>\n\n            <form action=\"\" class=\"miso_form1\">\n              <div class=\"input-group pore\">\n                <input type=\"text\" name=\"showkeycode\" id=\"\" class=\"inp\" placeholder=\"\" autocomplete=\"off\">\n                <button class=\"sub\" type=\"submit\"></button>\n              </div>\n            </form>\n          </div>\n\n          <div class=\"navBtnm flex\">\n            <a href=\"javascript:;\" class=\"navbtn\">\n              <button class=\"menu-btn\">\n                <span class=\"line-1\"></span>\n                <span class=\"line-2\"></span>\n                <span class=\"line-3\"></span>\n              </button>\n            </a>\n          </div>\n        </div>\n      </header>\n\n     <div class=\"site-menu\" data-lenis-prevent=\"\">\n        <div class=\"site-menu-main px-container\">\n            <div class=\"bottom\">\n                <div class=\"wp\">\n                    <div class=\"hd\">\n                        <ul class=\"ul flex\">\n                                                       <li>\n                                <a href=\"../../index.htm\">首页</a>\n                                <div class=\"sub-nav\">\n                                    <div class=\"sub-nav-h\">\n                      <a href=\"../../index/rdxw.htm\">热点新闻</a>\n                                       \n                      <a href=\"../../index/jzyg.htm\">讲座预告</a>\n                                       \n                                    </div>\n                                </div>\n                            </li>\n                            \n                            <li>\n                                <a href=\"../../yxgk/yxjj.htm\">院系概况</a>\n                                <div class=\"sub-nav\">\n                                    <div class=\"sub-nav-h\">\n                      <a href=\"../../yxgk/yxjj.htm\">院系简介</a>\n                                       \n                      <a href=\"../../yxgk/yzjy.htm\">院长寄语</a>\n                                       \n                      <a href=\"../../yxgk/xrld.htm\">现任领导</a>\n                                       \n                      <a href=\"../../yxgk/yxyg.htm\">院系沿革</a>\n                                       \n                                    </div>\n                                </div>\n                            </li>\n                            \n                            <li>\n                                <a href=\"../../xwdt/yxdt.htm\">新闻动态</a>\n                                <div class=\"sub-nav\">\n                                    <div class=\"sub-nav-h\">\n                      <a href=\"../../xwdt/yxdt.htm\">院系动态</a>\n                                       \n                      <a href=\"../../xwdt/mtjj.htm\">媒体聚焦</a>\n                                       \n                                    </div>\n                                </div>\n                            </li>\n                            \n                            <li>\n                                <a href=\"../../rydw.htm\">人员队伍</a>\n                                <div class=\"sub-nav\">\n                                    <div class=\"sub-nav-h\">\n                      <a href=\"../../rydw.htm#sz1\">全职教师</a>\n                                       \n                      <a href=\"../../rydw.htm#sz8\">研究系列</a>\n                                       \n                      <a href=\"../../rydw.htm#sz2\">荣誉客座</a>\n                                       \n                      <a href=\"../../rydw.htm#sz3\">兼职教师</a>\n                                       \n                      <a href=\"../../rydw.htm#sz4\">博士后</a>\n                                       \n                      <a href=\"../../rydw.htm#sz5\">行政人员</a>\n                                       \n                      <a href=\"../../rydw.htm#sz6\">实验员</a>\n                                       \n                                    </div>\n                                </div>\n                            </li>\n                            \n                            <li>\n                                <a href=\"../../ybbks/ybpy.htm\">姚班本科生</a>\n                                <div class=\"sub-nav\">\n                                    <div class=\"sub-nav-h\">\n                      <a href=\"../../ybbks/ybpy.htm\">姚班培养</a>\n                                       \n                      <a href=\"../../ybbks/ybzs.htm\">姚班招生</a>\n                                       \n                                    </div>\n                                </div>\n                            </li>\n                            \n                            <li>\n                                <a href=\"../../yjs/yjspy.htm\">研究生</a>\n                                <div class=\"sub-nav\">\n                                    <div class=\"sub-nav-h\">\n                      <a href=\"../../yjs/yjspy.htm\">研究生培养</a>\n                                       \n                      <a href=\"../../yjs/yjszs.htm\">研究生招生</a>\n                                       \n                                    </div>\n                                </div>\n                            </li>\n                            \n                            <li>\n                                <a href=\"../../lzzx/zxjj.htm\">量子中心</a>\n                                <div class=\"sub-nav\">\n                                    <div class=\"sub-nav-h\">\n                      <a href=\"../../lzzx/zxjj.htm\">中心简介</a>\n                                       \n                      <a href=\"../../lzzx/zxjj.htm#lzsz\">师资力量</a>\n                                       \n                      <a href=\"../../lzzx/kygk.htm\">科研概况</a>\n                                       \n                      <a href=\"../../lzzx/lzzxdt.htm\">量子中心动态</a>\n                                       \n                                    </div>\n                                </div>\n                            </li>\n                            \n                            <li>\n                                <a href=\"../../kxyj/zzjg.htm\">科学研究</a>\n                                <div class=\"sub-nav\">\n                                    <div class=\"sub-nav-h\">\n                      <a href=\"../../kxyj/zzjg.htm\">组织架构</a>\n                                       \n                      <a href=\"../../kxyj/ktzjs.htm\">课题组介绍</a>\n                                       \n                      <a href=\"../../kxyj/yjdt.htm\">研究动态</a>\n                                       \n                      <a href=\"../../kxyj/kyjb.htm\">科研简报</a>\n                                       \n                                    </div>\n                                </div>\n                            </li>\n                            \n                            <li>\n                                <a href=\"../../xysh/xshd/nljlb.htm\">校园生活</a>\n                                <div class=\"sub-nav\">\n                                    <div class=\"sub-nav-h\">\n                      <a href=\"../../xysh/xshd/nljlb.htm\">学生活动</a>\n                                       \n                      <a href=\"../../xysh/yszx.htm\">衣食住行</a>\n                                       \n                                    </div>\n                                </div>\n                            </li>\n                            \n                            <li>\n                                <a href=\"../../gkzp.htm\">公开招聘</a>\n                                <div class=\"sub-nav\">\n                                    <div class=\"sub-nav-h\">\n                      <a href=\"../../gkzp/jszp.htm\">教师招聘</a>\n                                       \n                      <a href=\"../../gkzp/bshzp.htm\">博士后招聘</a>\n                                       \n                      <a href=\"../../gkzp/zyzp.htm\">职员招聘</a>\n                                       \n                                    </div>\n                                </div>\n                            </li>\n                            \n                            <li>\n                                <a href=\"../../xypd/xyfc.htm\">校友频道</a>\n                                <div class=\"sub-nav\">\n                                    <div class=\"sub-nav-h\">\n                      <a href=\"../../xypd/xyfc.htm\">校友风采</a>\n                                       \n                      <a href=\"../../xypd/xyjlb/zqjlb.htm\">校友俱乐部</a>\n                                       \n                      <a href=\"../../xypd/xyjz.htm\">校友捐赠</a>\n                                       \n                                    </div>\n                                </div>\n                            </li>\n                            \n\n\n</ul>\n</div>\n                </div>\n            </div>\n        </div>\n      </div>\n\n  <div class=\"n_container n_b1 rydw_d\">\n  <div id=\"waveContainer\"><canvas width=\"1280\" height=\"360\" style=\"width: 1280px; height: 360px;\"></canvas></div>\n    <div class=\"wp\">\n      <div class=\"n_yj\"><h4>\n\n\n\n<font>人员队伍</font></h4></div>\n      <div class=\"n_ej\">\n        <ul class=\"flex\">\n          \n    <li class=\"on\"><a href=\"../../rydw.htm#sz1\" class=\"a\">全职教师</a></li>\n           \n    <li><a href=\"../../rydw.htm#sz8\" class=\"a\">研究系列</a></li>\n           \n    <li><a href=\"../../rydw.htm#sz2\" class=\"a\">荣誉客座</a></li>\n           \n    <li><a href=\"../../rydw.htm#sz3\" class=\"a\">兼职教师</a></li>\n           \n    <li><a href=\"../../rydw.htm#sz4\" class=\"a\">博士后</a></li>\n           \n    <li><a href=\"../../rydw.htm#sz5\" class=\"a\">行政人员</a></li>\n           \n    <li><a href=\"../../rydw.htm#sz6\" class=\"a\">实验员</a></li>\n           \n</ul>\n\n      </div>\n    </div>\n    <div class=\"fl1 n_pad1\">\n      <div class=\"wp flexjs\">\n        <div class=\"left\">\n          <div>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<script language=\"javascript\" src=\"/system/resource/js/ajax.js\"></script><script language=\"javascript\">_getBatchClickTimes('null',2110873241,'wbnews','u10')</script>\n<script>function seeContenta10(contentid,size,displayid){    document.getElementById(contentid).innerHTML = '[';    for(var i=0;i<=size;i++){        var allcontentid = contentid+(i+1);        if(allcontentid==displayid){            document.getElementById(contentid).innerHTML += \" <span id='\"+allcontentid+\"' name='\"+allcontentid+\"'  >\"+(i+1)+\"</span> \";            document.getElementById(displayid).style.display = 'block';        }else{            document.getElementById(contentid).innerHTML += \" <span style='cursor:pointer' id='\"+allcontentid+\"' name='\"+allcontentid+\"' onclick=seeContenta10('\"+contentid+\"','\"+size+\"','\"+allcontentid+\"')  >\"+(i+1)+\"</span> \";            document.getElementById(allcontentid).style.display = 'none';        }    }    document.getElementById(contentid).innerHTML += ']';}</script>\n<script language=\"javascript\" src=\"/system/resource/js/news/mp4video.js\"></script>\n<script>_addDynClicks('wbnews',2110873241,3936)</script>\n\n<div class=\"box box0 box1 flex\">\n            <div class=\"imgBox trans\">\n              <div class=\"img light\">\n                <img src=\"../../zpyx/yeli.jpg\" alt=\"\">\n              </div>\n            </div>\n            <div class=\"con con1\">\n              <h3>弋力</h3>\n              <h6></h6>\n              <p>助理教授</p>\n              <p>三维计算机视觉，三维深度学习，机器人感知，计算机图形学，几何处理</p>\n            </div>\n          </div>\n\n    <div class=\"box box0 box2 aos-init aos-animate\" data-aos=\"fade-up\">\n       \n         <h3 class=\"h3-5 flex\"><img src=\"../../images/h3-5.png\" alt=\"\">About</h3>\n                <div class=\"arc-con arc-cons1\"><div class=\"v_news_content\">\n<div>\n <p class=\"vsbcontent_start\">I am an Assistant Professor in the <a href=\"https://iiis.tsinghua.edu.cn/\">Institute for Interdisciplinary Information Sciences (IIIS)</a> at Tsinghua University. I received my Ph.D. from Stanford University, advised by Professor <a href=\"http://geometry.stanford.edu/member/guibas/index.html\">Leonidas J. Guibas</a>. And I spent a wonderful time at Google as a Research Scientist after graduation, working closely with Professor <a href=\"https://www.cs.princeton.edu/~funk/\">Thomas Funkhouser</a>. Prior to joining Stanford, I got my bachelor's degree in Electronic Engineering from <a href=\"http://www.tsinghua.edu.cn/\">Tsinghua University</a>.</p>\n <p class=\"vsbcontent_end\">My recent research interests focus on 3D perception, human-robot interaction and embodied AI, with the goal of equipping robotic agent with the ability of understanding and interacting with the 3D world.</p>\n <h3></h3>\n</div>\n</div></div>\n \n  </div>\n  \n    <div class=\"box box0 box3 aos-init\" data-aos=\"fade-up\">\n       \n         <h3 class=\"h3-5 flex\"><img src=\"../../images/h3-5.png\" alt=\"\">Recruiting</h3>\n                <div class=\"arc-con arc-cons1\"><p class=\"vsbcontent_start\">I am actively looking for motivated visiting students, interns, PhDs, and postdocs. Please feel free to email me if you are interested.</p>\n<p>§ For PhD applicants, please contact me at least half a year prior to your application.</p>\n<p class=\"vsbcontent_end\">§ For visiting students or research interns, we have openings for long-term internship (six months or longer). Both undergraduate and graduate students are welcomed. Please email me with your CV and transcript to apply.</p></div>\n \n  </div>\n  \n    <div class=\"box box0 box3 aos-init\" data-aos=\"fade-up\">\n       \n         <h3 class=\"h3-5 flex\"><img src=\"../../images/h3-5.png\" alt=\"\">News</h3>\n                <div class=\"arc-con arc-cons1\"><p class=\"vsbcontent_start\">§ <b>NEW</b> [2025/02] Five papers accepted to CVPR 2025.</p>\n<p>§ <b>NEW</b> [2025/01] Two papers accepted to ICLR 2025.</p>\n<p>§ <b>NEW</b> [2024/12] I am invited to be a speaker in <a href=\"https://rhobin-challenge.github.io/\">the third Workshop on Reconstruction of Human-Object Interactions (RHOBIN)</a> at CVPR 2025.</p>\n<p>§ <b>NEW</b> [2024/12] I am organizing <a href=\"https://humanoid-agents.github.io/\">the 1st Workshop on Humanoid Agents</a> at <a href=\"https://cvpr.thecvf.com/\">CVPR 2025</a>.</p>\n<p>§ <b>NEW</b> [2024/11] I am invited to be a speaker in <a href=\"https://dex-manipulation.github.io/corl2024/index.html\">Learning Robot Fine and Dexterous Manipulation Workshop</a> at CoRL 2024 (<a href=\"https://www.youtube.com/watch?v=rtHeKZJEzSg&amp;t=14867s\">video recording</a>).</p>\n<p>§ <b>NEW</b> [2024/11] Two papers accepted to 3DV 2025.</p>\n<p>§ [2024/09] One paper accepted to NeurIPS 2024.</p>\n<p>§ [2024/07] Three papers accepted to ECCV 2024 and one paper accepted to ACMMM 2024 as oral.</p>\n<p>§ [2024/03] Four papers accepted to CVPR 2024.</p>\n<p>§ [2024/01] Two papers accepted to ICRA 2024 with one also accepted to Robotics and Automation Letters (RA-L).</p>\n<p>§ [2024/01] Two papers accepted to ICLR 2024 with one as spotlight.</p>\n<p>§ [2023/12] Two papers accepted to AAAI 2024.</p>\n<p>§ [2023/07] Four papers accepted to ICCV 2023.</p>\n<p>§ [2023/06] I will serve as an Area Chair for <a href=\"https://cvpr.thecvf.com/Conferences/2024/\">CVPR 2024</a>.</p>\n<p>§ [2023/04] One paper accepted to ICML 2023 and one paper accepted to SIGGRAPH 2023.</p>\n<p>§ [2023/03] I serve as an Area Chair for <a href=\"https://nips.cc/\">NeurIPS 2023</a>.</p>\n<p>§ [2023/03] Seven papers accepted to CVPR 2023.</p>\n<p>§ [2023/01] Two papers accepted to ICLR 2023.</p>\n<p>§ [2023/01] Two papers accepted to AAAI 2023 as orals.</p>\n<p>§ [2022/10] I serve as an Area Chair for <a href=\"https://cvpr2023.thecvf.com/\">CVPR 2023</a>.</p>\n<p>§ [2022/09] One paper accepted to ECCV 2022 and one paper accepted to SIGGRAPH Asia 2022.</p>\n<p>§ [2022/03] Seven papers accepted to CVPR 2022.</p>\n<p>§ [2021/09] Two papers accepted to NeurIPS 2021 and one paper accepted to ICCV 2021.</p>\n<p>§ [2021/05] I serve as an Area Chair for <a href=\"http://cvpr2022.thecvf.com/\">CVPR 2022</a>.</p>\n<p>§ [2021/05] I am organizing <a href=\"https://iccv21-seai.github.io/\">The 1st Workshop on Simulation Technology for Embodied AI</a> at <a href=\"http://iccv2021.thecvf.com/home\">ICCV 2021</a>.</p>\n<p class=\"vsbcontent_end\">§ [2021/03] Two papers accepted at CVPR 2021 (one oral included).</p>\n<div></div></div>\n \n  </div>\n  \n    <div class=\"box box0 box3 aos-init\" data-aos=\"fade-up\">\n       \n         <h3 class=\"h3-5 flex\"><img src=\"../../images/h3-5.png\" alt=\"\">Recent Projects</h3>\n                <div class=\"arc-con arc-cons1\"><p class=\"vsbcontent_start\">*: equivalent contribution, †: corresponding author</p>\n<div class=\"scroll\"><table align=\"center\" style=\"border-collapse: collapse;\">\n <tbody>\n  <tr class=\"firstRow\">\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"161\" height=\"161\" src=\"/__local/3/6C/CD/FDD11B085BD59F194C9E3391B57_9E3C398E_1969D.png\" vsbhref=\"vurl\" vurl=\"/_vsl/36CCDFDD11B085BD59F194C9E3391B57/9E3C398E/1969D\" vheight=\"161\" vwidth=\"161\" orisrc=\"/__local/3/6C/CD/FDD11B085BD59F194C9E3391B57_9E3C398E_1969D.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Deep Object-Centric 3D Perception</a></p><p><b>Li Yi</b></p><p>Ph.D. Thesis</p><p><a href=\"https://searchworks.stanford.edu/view/13333376\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"200\" src=\"/__local/8/FA/A9/B455705648655FFA7B128123B0C_46C0A341_41369.png\" vsbhref=\"vurl\" vurl=\"/_vsl/8FAA9B455705648655FFA7B128123B0C/46C0A341/41369\" vheight=\"200\" vwidth=\"333\" orisrc=\"/__local/8/FA/A9/B455705648655FFA7B128123B0C_46C0A341_41369.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Learning Physics-Based Full-Body Human Reaching and Grasping from Brief Walking References</a></p><p>Yitang Li*, Mingxian Lin*, Zhuo Lin, Yipeng Deng, Yue Cao, <b>Li Yi†</b></p><p>CVPR 2025</p><p><a href=\"http://arxiv.org/abs/2503.07481\">PDF</a> <a href=\"https://liyitang22.github.io/phys-reach-grasp/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"172\" src=\"/__local/2/63/73/E277038FC9D6704E91E249B6A06_D7FD5327_38157.png\" vsbhref=\"vurl\" vurl=\"/_vsl/26373E277038FC9D6704E91E249B6A06/D7FD5327/38157\" vheight=\"172\" vwidth=\"333\" orisrc=\"/__local/2/63/73/E277038FC9D6704E91E249B6A06_D7FD5327_38157.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">CORE4D : A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement</a></p><p>Yun Liu*, Chengwen Zhang*, Ruofan Xing, Bingda Tang, Bowen Yang, <b>Li Yi†</b></p><p>CVPR 2025</p><p><a href=\"https://arxiv.org/abs/2406.19353\">PDF</a> <a href=\"https://core4d.github.io/\">Project</a> <a href=\"https://1drv.ms/f/s!Ap-t7dLl7BFUmHl9Une1E6FLsS4J?e=RLt0Fk\">Data</a> <a href=\"https://github.com/leolyliu/CORE4D-Instructions\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"316\" src=\"/__local/6/E0/89/FBE4476E53CC457532FC49A7796_8397280E_67065.png\" vsbhref=\"vurl\" vurl=\"/_vsl/6E089FBE4476E53CC457532FC49A7796/8397280E/67065\" vheight=\"316\" vwidth=\"333\" orisrc=\"/__local/6/E0/89/FBE4476E53CC457532FC49A7796_8397280E_67065.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">MobileH2R: Learning Generalizable Human to Mobile Robot Handover Exclusively from Scalable and Diverse Synthetic Data</a></p><p>Zifan Wang*, Ziqing Chen*, Junyu Chen*, Jilong Wang, Yuxin Yang, Yunze Liu, Xueyi Liu, He Wang, <b>Li Yi†</b></p><p>CVPR 2025</p><p><a href=\"https://arxiv.org/abs/2501.04595\">PDF</a> <a href=\"https://www.youtube.com/watch?v=LiT1y7zUnz8\">Video</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"246\" height=\"267\" src=\"/__local/D/95/18/52A41D954FE51C7756EBC0BF6C2_38C6DD0D_4054F.png\" vsbhref=\"vurl\" vurl=\"/_vsl/D951852A41D954FE51C7756EBC0BF6C2/38C6DD0D/4054F\" vheight=\"267\" vwidth=\"246\" orisrc=\"/__local/D/95/18/52A41D954FE51C7756EBC0BF6C2_38C6DD0D_4054F.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">MAP: Unleashing Hybrid Mamba-Transformer Vision Backbone's Potential with Masked Autoregressive Pretraining</a></p><p>Yunze Liu, <b>Li Yi†</b></p><p>CVPR 2025</p><p><a href=\"https://arxiv.org/abs/2410.00871\">PDF</a> <a href=\"https://github.com/yunzeliu/MAP\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"149\" src=\"/__local/B/62/D6/CF0AA121E2B29FA9C92DB41922C_AA4B3DE6_3096B.png\" vsbhref=\"vurl\" vurl=\"/_vsl/B62D6CF0AA121E2B29FA9C92DB41922C/AA4B3DE6/3096B\" vheight=\"149\" vwidth=\"333\" orisrc=\"/__local/B/62/D6/CF0AA121E2B29FA9C92DB41922C_AA4B3DE6_3096B.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model</a></p><p>Mingju Gao*, Yike Pan*, Huan-ang Gao*, Zongzheng Zhang, Wenyi Li, Hao Dong, Hao Tang, <b>Li Yi</b>, Hao Zhao†</p><p>CVPR 2025</p><p><a href=\"https://arxiv.org/abs/2503.19913\">PDF</a> <a href=\"https://partrm.c7w.tech/\">Project</a> <a href=\"https://huggingface.co/GasaiYU/PartRM/tree/main\">Dataset&amp;Models</a> <a href=\"https://github.com/GasaiYU/PartRM\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><br></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References</a></p><p>Xueyi Liu, Jianibieke Adalibieke, Qianwei Han, Yuzhe Qin, <b>Li Yi†</b></p><p>ICLR 2025</p><p><a href=\"https://arxiv.org/abs/2502.09614\">PDF</a> <a href=\"https://meowuu7.github.io/DexTrack/\">Project</a> <a href=\"https://github.com/Meowuu7/DexTrack\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"104\" src=\"/__local/0/29/AA/8EDEB703CEF9D8B2417002D04FF_843B7B42_21EB5.png\" vsbhref=\"vurl\" vurl=\"/_vsl/029AA8EDEB703CEF9D8B2417002D04FF/843B7B42/21EB5\" vheight=\"104\" vwidth=\"333\" orisrc=\"/__local/0/29/AA/8EDEB703CEF9D8B2417002D04FF_843B7B42_21EB5.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation</a></p><p>Yecheng Wu*, Zhuoyang Zhang*, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, <b>Li Yi</b>, Song Han, Yao Lu</p><p>ICLR 2025</p><p><a href=\"https://arxiv.org/abs/2409.04429\">PDF</a> <a href=\"https://hanlab.mit.edu/projects/vila-u\">Project</a> <a href=\"https://github.com/mit-han-lab/vila-u\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"151\" src=\"/__local/1/61/CD/1E4BF0033309D4C981F49683876_E97A869C_313DA.png\" vsbhref=\"vurl\" vurl=\"/_vsl/161CD1E4BF0033309D4C981F49683876/E97A869C/313DA\" vheight=\"151\" vwidth=\"333\" orisrc=\"/__local/1/61/CD/1E4BF0033309D4C981F49683876_E97A869C_313DA.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Interactive Humanoid: Online Full-Body Motion Reaction Synthesis with Social Affordance Canonicalization and Forecasting</a></p><p>Yunze Liu, Changxi Chen, <b>Li Yi†</b></p><p>3DV 2025</p><p><a href=\"https://arxiv.org/abs/2312.08983\">PDF</a> <a href=\"https://yunzeliu.github.io/iHuman/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"170\" src=\"/__local/5/CC/34/793525C13DC17DA4456D030E233_2BD33D29_376ED.png\" vsbhref=\"vurl\" vurl=\"/_vsl/5CC34793525C13DC17DA4456D030E233/2BD33D29/376ED\" vheight=\"170\" vwidth=\"333\" orisrc=\"/__local/5/CC/34/793525C13DC17DA4456D030E233_2BD33D29_376ED.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">ImOV3D: Learning Open Vocabulary Point Clouds 3D Object Detection from Only 2D Images</a></p><p>Timing Yang*, Yuanliang Ju*, <b>Li Yi†</b></p><p>NeurIPS 2024</p><p><a href=\"https://arxiv.org/pdf/2410.24001v1\">PDF</a> <a href=\"https://github.com/yangtiming/imov3d\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"307\" src=\"/__local/1/4D/F7/AEF73393125CCC35092A0FA938A_1C44030B_64170.png\" vsbhref=\"vurl\" vurl=\"/_vsl/14DF7AEF73393125CCC35092A0FA938A/1C44030B/64170\" vheight=\"307\" vwidth=\"333\" orisrc=\"/__local/1/4D/F7/AEF73393125CCC35092A0FA938A_1C44030B_64170.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">ShapeLLM: Universal 3D Object Understanding for Embodied Interaction</a></p><p>Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, He Wang, <b>Li Yi†</b>, Kaisheng Ma†</p><p>ECCV 2024</p><p><a href=\"https://arxiv.org/abs/2402.17766\">PDF</a> <a href=\"https://qizekun.github.io/shapellm/\">Project</a> <a href=\"https://github.com/qizekun/ShapeLLM\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><br></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">QuasiSim: Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer</a></p><p>Xueyi Liu, Kangbo Lyu, Jieqiong Zhang, Tao Du, <b>Li Yi†</b></p><p>ECCV 2024</p><p><a href=\"https://arxiv.org/abs/2404.07988\">PDF</a> <a href=\"https://meowuu7.github.io/QuasiSim/\">Project</a> <a href=\"https://github.com/Meowuu7/QuasiSim\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"138\" src=\"/__local/4/C1/B0/4CBEFF5DC89D3B26271BE31AA3A_9087ADF8_2D00C.png\" vsbhref=\"vurl\" vurl=\"/_vsl/4C1B04CBEFF5DC89D3B26271BE31AA3A/9087ADF8/2D00C\" vheight=\"138\" vwidth=\"333\" orisrc=\"/__local/4/C1/B0/4CBEFF5DC89D3B26271BE31AA3A_9087ADF8_2D00C.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models</a></p><p>Zhikai Zhang, Yitang Li, Haofeng Huang, Mingxian Lin, <b>Li Yi†</b></p><p>ECCV 2024</p><p><a href=\"https://arxiv.org/abs/2406.10740\">PDF</a> <a href=\"https://zzk273.github.io/freemotion.github.io/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"130\" src=\"/__local/9/6A/72/53DA81BEACEAE0D356C1603ADA6_DB5CBF68_2A658.png\" vsbhref=\"vurl\" vurl=\"/_vsl/96A7253DA81BEACEAE0D356C1603ADA6/DB5CBF68/2A658\" vheight=\"130\" vwidth=\"333\" orisrc=\"/__local/9/6A/72/53DA81BEACEAE0D356C1603ADA6_DB5CBF68_2A658.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">PhysReaction: Physically Plausible Real-Time Humanoid Reaction Synthesis via Forward Dynamics Guided 4D Imitation</a></p><p>Yunze Liu, Changxi Chen, Chenjing Ding, <b>Li Yi†</b></p><p>ACMMM 2024 (Oral, top 3.9%)</p><p><a href=\"https://arxiv.org/abs/2404.01081\">PDF</a> <a href=\"https://yunzeliu.github.io/PhysReaction/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"187\" src=\"/__local/A/F9/3A/5D068A6BB8FEA8ECFB676E5AC27_5B237E19_3CF9B.png\" vsbhref=\"vurl\" vurl=\"/_vsl/AF93A5D068A6BB8FEA8ECFB676E5AC27/5B237E19/3CF9B\" vheight=\"187\" vwidth=\"333\" orisrc=\"/__local/A/F9/3A/5D068A6BB8FEA8ECFB676E5AC27_5B237E19_3CF9B.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable Simulation, Demonstration, and Imitation</a></p><p>Zifan Wang*, Junyu Chen*, Ziqing Chen, Pengwei Xie, Rui Chen, <b>Li Yi†</b></p><p>CVPR 2024</p><p><a href=\"https://arxiv.org/abs/2401.00929\">PDF</a> <a href=\"https://genh2r.github.io/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"144\" src=\"/__local/1/B9/F5/B98BBA35101C3E7FD8A9897A700_CB46901C_2EF56.png\" vsbhref=\"vurl\" vurl=\"/_vsl/1B9F5B98BBA35101C3E7FD8A9897A700/CB46901C/2EF56\" vheight=\"144\" vwidth=\"333\" orisrc=\"/__local/1/B9/F5/B98BBA35101C3E7FD8A9897A700_CB46901C_2EF56.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding</a></p><p>Yun Liu, Haolin Yang, Xu Si, Ling Liu, Zipeng Li, Yuxiang Zhang, Yebin Liu, <b>Li Yi†</b></p><p>CVPR 2024</p><p><a href=\"https://arxiv.org/abs/2401.08399\">PDF</a> <a href=\"https://taco2024.github.io/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"267\" height=\"288\" src=\"/__local/3/62/20/C655FA633FA4018BC56A3A31598_5342FA13_4B4E2.png\" vsbhref=\"vurl\" vurl=\"/_vsl/36220C655FA633FA4018BC56A3A31598/5342FA13/4B4E2\" vheight=\"288\" vwidth=\"267\" orisrc=\"/__local/3/62/20/C655FA633FA4018BC56A3A31598_5342FA13_4B4E2.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">GenN2N: Generative NeRF2NeRF Translation</a></p><p>Xiangyue Liu, Han Xue, Kunming Luo, Ping Tan†, <b>Li Yi†</b></p><p>CVPR 2024</p><p><a href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_GenN2N_Generative_NeRF2NeRF_Translation_CVPR_2024_paper.pdf\">PDF</a> <a href=\"https://xiangyueliu.github.io/GenN2N/\">Project</a> <a href=\"https://github.com/Lxiangyue/GenN2N\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"132\" src=\"/__local/2/D3/2A/1E33F4722F9AD894211867AF044_F1A08120_2B0C2.png\" vsbhref=\"vurl\" vurl=\"/_vsl/2D32A1E33F4722F9AD894211867AF044/F1A08120/2B0C2\" vheight=\"132\" vwidth=\"333\" orisrc=\"/__local/2/D3/2A/1E33F4722F9AD894211867AF044_F1A08120_2B0C2.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Physics-aware Hand-object Interaction Denoising</a></p><p>Haowen Luo, Yunze Liu, <b>Li Yi†</b></p><p>CVPR 2024</p><p><a href=\"https://arxiv.org/abs/2405.11481\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"295\" height=\"200\" src=\"/__local/1/02/15/C315E83C6A2B784A5CF9873E06A_214F6E0A_39C74.png\" vsbhref=\"vurl\" vurl=\"/_vsl/10215C315E83C6A2B784A5CF9873E06A/214F6E0A/39C74\" vheight=\"200\" vwidth=\"295\" orisrc=\"/__local/1/02/15/C315E83C6A2B784A5CF9873E06A_214F6E0A_39C74.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">DreamLLM: Synergistic Multimodal Comprehension and Creation</a></p><p>Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma†, <b>Li Yi†</b></p><p>ICLR 2024 (Spotlight, top 5%)</p><p><a href=\"https://arxiv.org/abs/2309.11499\">PDF</a> <a href=\"https://dreamllm.github.io/\">Project</a> <a href=\"https://github.com/RunpeiDong/DreamLLM\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"355\" height=\"200\" src=\"/__local/F/6D/5D/8CE90CF9C3EB001235DE0FEB757_6E715C55_45841.png\" vsbhref=\"vurl\" vurl=\"/_vsl/F6D5D8CE90CF9C3EB001235DE0FEB757/6E715C55/45841\" vheight=\"200\" vwidth=\"355\" orisrc=\"/__local/F/6D/5D/8CE90CF9C3EB001235DE0FEB757_6E715C55_45841.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion</a></p><p>Xueyi Liu, <b>Li Yi†</b></p><p>ICLR 2024</p><p><a href=\"https://openreview.net/pdf?id=FvK2noilxT\">PDF</a> <a href=\"https://meowuu7.github.io/GeneOH-Diffusion/\">Project</a> <a href=\"https://openreview.net/forum?id=FvK2noilxT\">OpenReview</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"323\" height=\"134\" src=\"/__local/8/32/A1/C22CE387A6B9D85D9930864ECAE_7500A7B0_2A63C.png\" vsbhref=\"vurl\" vurl=\"/_vsl/832A1C22CE387A6B9D85D9930864ECAE/7500A7B0/2A63C\" vheight=\"134\" vwidth=\"323\" orisrc=\"/__local/8/32/A1/C22CE387A6B9D85D9930864ECAE_7500A7B0_2A63C.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Enhancing Generalizable 6D Pose Tracking of an In-Hand Object with Tactile Sensing</a></p><p>Yun Liu*, Xiaomeng Xu*, Weihang Chen, Haocheng Yuan, He Wang, Jing Xu, Rui Chen, <b>Li Yi†</b></p><p>RA-L with a presentation at ICRA 2024</p><p><a href=\"https://arxiv.org/abs/2210.04026\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"300\" height=\"161\" src=\"/__local/3/26/29/2081496F92BFB9A6D92F7498243_D7679D82_2F4D7.png\" vsbhref=\"vurl\" vurl=\"/_vsl/326292081496F92BFB9A6D92F7498243/D7679D82/2F4D7\" vheight=\"161\" vwidth=\"300\" orisrc=\"/__local/3/26/29/2081496F92BFB9A6D92F7498243_D7679D82_2F4D7.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">CrossVideo: Self-supervised Cross-modal Contrastive Learning for Point Cloud Video Understanding</a></p><p>Yunze Liu, Changxi Chen, Zifan Wang, <b>Li Yi†</b></p><p>ICRA 2024</p><p><a href=\"https://arxiv.org/abs/2401.09057\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"72\" src=\"/__local/8/CE/FD/DF338060F5D30F28AC117F44701_7E33390A_177CF.png\" vsbhref=\"vurl\" vurl=\"/_vsl/8CEFDDF338060F5D30F28AC117F44701/7E33390A/177CF\" vheight=\"72\" vwidth=\"333\" orisrc=\"/__local/8/CE/FD/DF338060F5D30F28AC117F44701_7E33390A_177CF.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Semantic Complete Scene Forecasting from a 4D Dynamic Point Cloud Sequence</a></p><p>Zifan Wang*, Zhuorui Ye*, Haoran Wu*, Junyu Chen, <b>Li Yi†</b></p><p>AAAI 2024</p><p><a href=\"https://arxiv.org/abs/2312.08054\">PDF</a> <a href=\"https://scsfnet.github.io/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"125\" src=\"/__local/C/1C/FF/CD542919EF9D435DEFAF7C56916_242FC857_28C43.png\" vsbhref=\"vurl\" vurl=\"/_vsl/C1CFFCD542919EF9D435DEFAF7C56916/242FC857/28C43\" vheight=\"125\" vwidth=\"333\" orisrc=\"/__local/C/1C/FF/CD542919EF9D435DEFAF7C56916_242FC857_28C43.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Full-Body Motion Reconstruction with Sparse Sensing from Graph Perspective</a></p><p>Feiyu Yao, Zongkai Wu†, <b>Li Yi†</b></p><p>AAAI 2024</p><p><a href=\"https://arxiv.org/abs/2401.11783\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"86\" src=\"/__local/4/85/A2/39C91F7DB4AC9E1358A6A4D1832_44BD69C9_1C0D2.png\" vsbhref=\"vurl\" vurl=\"/_vsl/485A239C91F7DB4AC9E1358A6A4D1832/44BD69C9/1C0D2\" vheight=\"86\" vwidth=\"333\" orisrc=\"/__local/4/85/A2/39C91F7DB4AC9E1358A6A4D1832_44BD69C9_1C0D2.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">NSM4D: Neural Scene Model Based Online 4D Point Cloud Sequence Understanding</a></p><p>Yuhao Dong*, Zhuoyang Zhang*, Yunze Liu, <b>Li Yi†</b></p><p>arXiv:2310.08326 [cs.CV], Oct 2023</p><p><a href=\"https://arxiv.org/abs/2310.08326\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"284\" height=\"161\" src=\"/__local/C/FF/F8/9503194CF320EA3537748649937_80DDA3F3_2CC8B.png\" vsbhref=\"vurl\" vurl=\"/_vsl/CFFF89503194CF320EA3537748649937/80DDA3F3/2CC8B\" vheight=\"161\" vwidth=\"284\" orisrc=\"/__local/C/FF/F8/9503194CF320EA3537748649937_80DDA3F3_2CC8B.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">TransTouch: Learning Transparent Objects Depth Sensing Through Sparse Touches</a></p><p>Liuyu Bian, Pengyang Shi, Weihang Chen, Jing Xu, <b>Li Yi†</b>, Rui Chen†</p><p>IROS 2023</p><p><a href=\"https://arxiv.org/pdf/2309.09427\">PDF</a> <a href=\"https://youtu.be/WbkK1TZfy3M\">Video</a> <a href=\"https://github.com/ritsu-a/transtouch\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"267\" height=\"161\" src=\"/__local/F/C7/8E/2F35A7D61965B0BCDBBC3E9B367_6C0CA54D_2A1BB.png\" vsbhref=\"vurl\" vurl=\"/_vsl/FC78E2F35A7D61965B0BCDBBC3E9B367/6C0CA54D/2A1BB\" vheight=\"161\" vwidth=\"267\" orisrc=\"/__local/F/C7/8E/2F35A7D61965B0BCDBBC3E9B367_6C0CA54D_2A1BB.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">LeaF: Learning Frames for 4D Point Cloud Sequence Understanding</a></p><p>Yunze Liu, Junyu Chen, Zekai Zhang, <b>Li Yi†</b></p><p>ICCV 2023</p><p><a href=\"https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_LeaF_Learning_Frames_for_4D_Point_Cloud_Sequence_Understanding_ICCV_2023_paper.pdf\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"278\" height=\"134\" src=\"/__local/3/25/DC/F200B75AAE7C6C31109E25373DC_3C06F055_247DB.png\" vsbhref=\"vurl\" vurl=\"/_vsl/325DCF200B75AAE7C6C31109E25373DC/3C06F055/247DB\" vheight=\"134\" vwidth=\"278\" orisrc=\"/__local/3/25/DC/F200B75AAE7C6C31109E25373DC_3C06F055_247DB.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation</a></p><p>Xueyi Liu, Bin Wang, He Wang, <b>Li Yi†</b></p><p>ICCV 2023</p><p><a href=\"https://arxiv.org/abs/2308.10898\">PDF</a> <a href=\"https://meowuu7.github.io/few-arti-obj-gen/\">Project</a> <a href=\"https://github.com/Meowuu7/few-arti-gen\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"317\" height=\"134\" src=\"/__local/9/1A/BA/57174AB1680086C6A71B7F0616E_8333BBAB_299A0.png\" vsbhref=\"vurl\" vurl=\"/_vsl/91ABA57174AB1680086C6A71B7F0616E/8333BBAB/299A0\" vheight=\"134\" vwidth=\"317\" orisrc=\"/__local/9/1A/BA/57174AB1680086C6A71B7F0616E_8333BBAB_299A0.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning</a></p><p>Weikang Wan*, Haoran Geng*, Yun Liu, Zikang Shan, Yaodong Yang, <b>Li Yi</b>, He Wang</p><p>ICCV 2023</p><p><a href=\"https://arxiv.org/abs/2304.00464\">PDF</a> <a href=\"https://pku-epic.github.io/UniDexGrasp++/\">Project</a> <a href=\"https://github.com/PKU-EPIC/UniDexGrasp2\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"275\" height=\"134\" src=\"/__local/F/B8/47/FC0B3052A66CFC443F4EC527ED0_DA1CEE82_24193.png\" vsbhref=\"vurl\" vurl=\"/_vsl/FB847FC0B3052A66CFC443F4EC527ED0/DA1CEE82/24193\" vheight=\"134\" vwidth=\"275\" orisrc=\"/__local/F/B8/47/FC0B3052A66CFC443F4EC527ED0_DA1CEE82_24193.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">3D Implicit Transporter for Temporally Consistent Keypoint Discovery</a></p><p>Chengliang Zhong, Yuhang Zheng, Yupeng Zheng, Hao Zhao, <b>Li Yi</b>, Xiaodong Mu, Ling Wang, Pengfei Li, Guyue Zhou, Chao Yang, Xinliang Zhang, Jian Zhao</p><p>ICCV 2023</p><p><a href=\"https://openaccess.thecvf.com/content/ICCV2023/papers/Zhong_3D_Implicit_Transporter_for_Temporally_Consistent_Keypoint_Discovery_ICCV_2023_paper.pdf\">PDF</a> <a href=\"https://github.com/zhongcl-thu/3D-Implicit-Transporter\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"272\" height=\"161\" src=\"/__local/4/C2/78/7F519820827AA7FD1EA7865F809_7F2C4E25_2AE4F.png\" vsbhref=\"vurl\" vurl=\"/_vsl/4C2787F519820827AA7FD1EA7865F809/7F2C4E25/2AE4F\" vheight=\"161\" vwidth=\"272\" orisrc=\"/__local/4/C2/78/7F519820827AA7FD1EA7865F809_7F2C4E25_2AE4F.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">ArrangementNet: Learning Scene Arrangements for Vectorized Indoor Scene Modeling</a></p><p>Jingwei Huang, Shanshan Zhang, Bo Duan, Yanfeng Zhang, Xiaoyang Guo, Mingwei Sun, <b>Li Yi</b></p><p>SIGGRAPH 2023 (Journal Track)</p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"335\" height=\"134\" src=\"/__local/0/2F/F6/BCA6C8CC08D5EF14D125D9FC451_231FF8F5_2BF5C.png\" vsbhref=\"vurl\" vurl=\"/_vsl/02FF6BCA6C8CC08D5EF14D125D9FC451/231FF8F5/2BF5C\" vheight=\"134\" vwidth=\"335\" orisrc=\"/__local/0/2F/F6/BCA6C8CC08D5EF14D125D9FC451_231FF8F5_2BF5C.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining</a></p><p>Zekun Qi*, Runpei Dong*, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma†, <b>Li Yi†</b></p><p>ICML 2023</p><p><a href=\"https://arxiv.org/pdf/2302.02318\">PDF</a> <a href=\"https://github.com/qizekun/ReCon\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"299\" height=\"161\" src=\"/__local/1/2E/05/CA7F857098EAD0A66092C8148C1_D9730194_2F253.png\" vsbhref=\"vurl\" vurl=\"/_vsl/12E05CA7F857098EAD0A66092C8148C1/D9730194/2F253\" vheight=\"161\" vwidth=\"299\" orisrc=\"/__local/1/2E/05/CA7F857098EAD0A66092C8148C1_D9730194_2F253.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Complete-to-Partial 4D Distillation for Self-Supervised Point Cloud Sequence Representation Learning</a></p><p>Zhuoyang Zhang*, Yuhao Dong*, Yunze Liu, <b>Li Yi†</b></p><p>CVPR 2023</p><p><a href=\"https://arxiv.org/abs/2212.05330\">PDF</a> <a href=\"https://dongyh20.github.io/c2p.github.io/\">Project</a> <a href=\"https://github.com/dongyh20/C2P\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"324\" height=\"134\" src=\"/__local/7/85/83/572C6C5761B55C5B26DC1837C51_49A7F388_2A854.png\" vsbhref=\"vurl\" vurl=\"/_vsl/78583572C6C5761B55C5B26DC1837C51/49A7F388/2A854\" vheight=\"134\" vwidth=\"324\" orisrc=\"/__local/7/85/83/572C6C5761B55C5B26DC1837C51_49A7F388_2A854.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis</a></p><p>Juntian Zheng*, Lixing Fang*, Qingyuan Zheng*, Yun Liu, <b>Li Yi†</b></p><p>CVPR 2023</p><p><a href=\"https://arxiv.org/abs/2303.15469\">PDF</a> <a href=\"https://cams-hoi.github.io/\">Project</a> <a href=\"https://github.com/cams-hoi/CAMS\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"206\" height=\"161\" src=\"/__local/1/22/DF/B17C9FDC485C6898246381E1EDA_57B45DC0_20806.png\" vsbhref=\"vurl\" vurl=\"/_vsl/122DFB17C9FDC485C6898246381E1EDA/57B45DC0/20806\" vheight=\"161\" vwidth=\"206\" orisrc=\"/__local/1/22/DF/B17C9FDC485C6898246381E1EDA_57B45DC0_20806.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">JacobiNeRF: NeRF Shaping with Mutual Information Gradients</a></p><p>Xiaomeng Xu, Yanchao Yang, Kaichun Mo, Boxiao Pan, <b>Li Yi</b>, Leonidas J. Guibas</p><p>CVPR 2023</p><p><a href=\"https://arxiv.org/abs/2304.00341\">PDF</a> <a href=\"https://github.com/xxm19/jacobinerf\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"233\" height=\"161\" src=\"/__local/B/A4/02/A074A942EA92D4E70F28808BEEB_FCD183C0_24C0A.png\" vsbhref=\"vurl\" vurl=\"/_vsl/BA402A074A942EA92D4E70F28808BEEB/FCD183C0/24C0A\" vheight=\"161\" vwidth=\"233\" orisrc=\"/__local/B/A4/02/A074A942EA92D4E70F28808BEEB_FCD183C0_24C0A.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">GAPartNet: Learning Generalizable and Actionable Parts for Cross-Category Object Perception and Manipulation</a></p><p>Haoran Geng*, Helin Xu*, Chengyang Zhao*, Chao Xu, <b>Li Yi</b>, Siyuan Huang, He Wang</p><p>CVPR 2023</p><p><a href=\"https://arxiv.org/abs/2211.05272\">PDF</a> <a href=\"https://pku-epic.github.io/GAPartNet/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"293\" height=\"161\" src=\"/__local/D/96/26/53C84F1C90B959248E1673982E0_56115416_2E33B.png\" vsbhref=\"vurl\" vurl=\"/_vsl/D962653C84F1C90B959248E1673982E0/56115416/2E33B\" vheight=\"161\" vwidth=\"293\" orisrc=\"/__local/D/96/26/53C84F1C90B959248E1673982E0_56115416_2E33B.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy</a></p><p>Yinzhen Xu*, Weikang Wan*, Jialiang Zhang*, Haoran Liu*, Zikang Shan, Hao Shen, Ruicheng Wang, Haoran Geng, Yijia Weng, Jiayi Chen, Tengyu Liu, <b>Li Yi</b>, He Wang</p><p>CVPR 2023</p><p><a href=\"https://arxiv.org/abs/2303.00938\">PDF</a> <a href=\"https://pku-epic.github.io/UniDexGrasp/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"185\" height=\"161\" src=\"/__local/0/35/81/C40B97ECC81857A711305DA5A6A_BFB199EF_1D315.png\" vsbhref=\"vurl\" vurl=\"/_vsl/03581C40B97ECC81857A711305DA5A6A/BFB199EF/1D315\" vheight=\"161\" vwidth=\"185\" orisrc=\"/__local/0/35/81/C40B97ECC81857A711305DA5A6A_BFB199EF_1D315.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer</a></p><p>Xuanyao Chen*, Zhijian Liu*, Haotian Tang, <b>Li Yi</b>, Hang Zhao, Song Han</p><p>CVPR 2023</p><p><a href=\"https://arxiv.org/abs/2303.17605\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"280\" height=\"134\" src=\"/__local/4/7C/4E/3EC18AAB9B62BCC50A68A8CD6F5_D2A53BDA_24C0B.png\" vsbhref=\"vurl\" vurl=\"/_vsl/47C4E3EC18AAB9B62BCC50A68A8CD6F5/D2A53BDA/24C0B\" vheight=\"134\" vwidth=\"280\" orisrc=\"/__local/4/7C/4E/3EC18AAB9B62BCC50A68A8CD6F5_D2A53BDA_24C0B.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Semi-Weakly Supervised Object Kinematic Motion Prediction</a></p><p>Gengxin Liu, Qian Sun, Haibin Huang, Chongyang Ma, Yulan Guo, <b>Li Yi</b>, Hui Huang, Ruizhen Hu</p><p>CVPR 2023</p><p><a href=\"https://arxiv.org/abs/2303.17774\">PDF</a> <a href=\"https://github.com/GengxinLiu/SWMP\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"311\" height=\"134\" src=\"/__local/B/DD/5A/D1E418CE361A0414475CB4EFCB7_BA60D35B_28D0B.png\" vsbhref=\"vurl\" vurl=\"/_vsl/BDD5AD1E418CE361A0414475CB4EFCB7/BA60D35B/28D0B\" vheight=\"134\" vwidth=\"311\" orisrc=\"/__local/B/DD/5A/D1E418CE361A0414475CB4EFCB7_BA60D35B_28D0B.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance</a></p><p>Xueyi Liu, Ji Zhang, Ruizhen Hu, Haibin Huang, He Wang, <b>Li Yi†</b></p><p>ICLR 2023</p><p><a href=\"https://arxiv.org/abs/2302.14268\">PDF</a> <a href=\"https://equi-articulated-pose.github.io/\">Project</a> <a href=\"https://github.com/Meowuu7/equi-articulated-pose\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><br></td>\n   <td style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><br></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"302\" height=\"200\" src=\"/__local/A/FD/D0/564E3A5F9BA3AD32D70BB25ADB8_8EB5C283_3B260.png\" vsbhref=\"vurl\" vurl=\"/_vsl/AFDD0564E3A5F9BA3AD32D70BB25ADB8/8EB5C283/3B260\" vheight=\"200\" vwidth=\"302\" orisrc=\"/__local/A/FD/D0/564E3A5F9BA3AD32D70BB25ADB8_8EB5C283_3B260.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?</a></p><p>Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, <b>Li Yi†</b>, Kaisheng Ma†</p><p>ICLR 2023</p><p><a href=\"https://arxiv.org/abs/2212.08320\">PDF</a> <a href=\"https://github.com/RunpeiDong/ACT\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"331\" height=\"134\" src=\"/__local/8/FF/39/FB0B689E7456DD34A7B6CA9EF4C_9A3AB8E1_2B6FC.png\" vsbhref=\"vurl\" vurl=\"/_vsl/8FF39FB0B689E7456DD34A7B6CA9EF4C/9A3AB8E1/2B6FC\" vheight=\"134\" vwidth=\"331\" orisrc=\"/__local/8/FF/39/FB0B689E7456DD34A7B6CA9EF4C_9A3AB8E1_2B6FC.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Language-Assisted 3D Feature Learning for Semantic Scene Understanding</a></p><p>Junbo Zhang, Guofan Fan, Guanghan Wang, Zhengyuan Su, Kaisheng Ma†, <b>Li Yi†</b></p><p>AAAI 2023 (Oral Presentation)</p><p><a href=\"https://arxiv.org/abs/2211.14091\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"271\" height=\"134\" src=\"/__local/9/21/96/E0D6664BDF90B81664F4C8AB436_29CCF138_23927.png\" vsbhref=\"vurl\" vurl=\"/_vsl/92196E0D6664BDF90B81664F4C8AB436/29CCF138/23927\" vheight=\"134\" vwidth=\"271\" orisrc=\"/__local/9/21/96/E0D6664BDF90B81664F4C8AB436_29CCF138_23927.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild</a></p><p>Jiayi Chen*, Mi Yan*, Jiazhao Zhang, Yenzhen Xu, Xiaolong Li, Yijia Weng, <b>Li Yi</b>, Shuran Song, He Wang</p><p>AAAI 2023 (Oral Presentation)</p><p><a href=\"http://arxiv.org/abs/2209.12009\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"307\" height=\"161\" src=\"/__local/B/D0/8D/385397C78065D9BC6D3AF388A7C_0662B2A6_3067F.png\" vsbhref=\"vurl\" vurl=\"/_vsl/BD08D385397C78065D9BC6D3AF388A7C/0662B2A6/3067F\" vheight=\"161\" vwidth=\"307\" orisrc=\"/__local/B/D0/8D/385397C78065D9BC6D3AF388A7C_0662B2A6_3067F.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">MoRig: Motion-Aware Rigging of Character Meshes from Point Clouds</a></p><p>Zhan Xu, Yang Zhou, <b>Li Yi</b>, Evangelos Kalogerakis</p><p>SIGGRAPH Asia 2022</p><p><a href=\"https://zhan-xu.github.io/motion-rig/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"355\" height=\"134\" src=\"/__local/C/1E/FE/77ECB9273607C52454555BCF94B_4ABDB24F_2E954.png\" vsbhref=\"vurl\" vurl=\"/_vsl/C1EFE77ECB9273607C52454555BCF94B/4ABDB24F/2E954\" vheight=\"134\" vwidth=\"355\" orisrc=\"/__local/C/1E/FE/77ECB9273607C52454555BCF94B_4ABDB24F_2E954.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding</a></p><p>Hao Wen*, Yunze Liu*, Jingwei Huang, Bo Duan, <b>Li Yi†</b></p><p>ECCV 2022</p><p><a href=\"https://arxiv.org/abs/2208.00281\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"275\" height=\"161\" src=\"/__local/9/ED/BA/44A45EB90D8E9140000BDD0CC1A_EE7038A0_2B5DB.png\" vsbhref=\"vurl\" vurl=\"/_vsl/9EDBA44A45EB90D8E9140000BDD0CC1A/EE7038A0/2B5DB\" vheight=\"161\" vwidth=\"275\" orisrc=\"/__local/9/ED/BA/44A45EB90D8E9140000BDD0CC1A_EE7038A0_2B5DB.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction</a></p><p>Yunze Liu*, Yun Liu*, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, <b>Li Yi†</b></p><p>CVPR 2022</p><p><a href=\"https://arxiv.org/abs/2203.01577\">PDF</a> <a href=\"https://hoi4d.github.io/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"287\" height=\"161\" src=\"/__local/3/63/80/25B77870609CD5FAB45E9CE385D_2288028A_2D417.png\" vsbhref=\"vurl\" vurl=\"/_vsl/3638025B77870609CD5FAB45E9CE385D/2288028A/2D417\" vheight=\"161\" vwidth=\"287\" orisrc=\"/__local/3/63/80/25B77870609CD5FAB45E9CE385D_2288028A_2D417.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Rotationally Equivariant 3D Object Detection</a></p><p>Hong-Xing Yu, Jiajun Wu, <b>Li Yi†</b></p><p>CVPR 2022</p><p><a href=\"https://kovenyu.com/eon/static/yu2022eon.pdf\">PDF</a> <a href=\"https://kovenyu.com/eon/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"276\" height=\"161\" src=\"/__local/2/DE/89/84E43243AE1CA7330EC295A2FD3_50BC13F5_2B85F.png\" vsbhref=\"vurl\" vurl=\"/_vsl/2DE8984E43243AE1CA7330EC295A2FD3/50BC13F5/2B85F\" vheight=\"161\" vwidth=\"276\" orisrc=\"/__local/2/DE/89/84E43243AE1CA7330EC295A2FD3_50BC13F5_2B85F.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation</a></p><p>Xueyi Liu, Xiaomeng Xu, Anyi Rao, Chuang Gan, <b>Li Yi†</b></p><p>CVPR 2022</p><p><a href=\"https://arxiv.org/abs/2203.06558\">PDF</a> <a href=\"https://autogpart.github.io/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"206\" height=\"161\" src=\"/__local/F/E7/B5/E6553EA0EC82D46FAF849DC8205_67F870AB_20806.png\" vsbhref=\"vurl\" vurl=\"/_vsl/FE7B5E6553EA0EC82D46FAF849DC8205/67F870AB/20806\" vheight=\"161\" vwidth=\"206\" orisrc=\"/__local/F/E7/B5/E6553EA0EC82D46FAF849DC8205_67F870AB_20806.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance</a></p><p>Tianchen Zhao, Niansong Zhang, Xuefei Ning, He Wang, <b>Li Yi†</b>, Yu Wang</p><p>CVPR 2022</p><p><a href=\"https://arxiv.org/abs/2203.09887\">PDF</a> <a href=\"https://a-suozhang.xyz/codedvtr.github.io\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"98\" src=\"/__local/C/72/4E/E5854AA665BBFBD35D29F3C1C72_132823E8_1FF66.png\" vsbhref=\"vurl\" vurl=\"/_vsl/C724EE5854AA665BBFBD35D29F3C1C72/132823E8/1FF66\" vheight=\"98\" vwidth=\"333\" orisrc=\"/__local/C/72/4E/E5854AA665BBFBD35D29F3C1C72_132823E8_1FF66.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Multi-Robot Active Mapping via Neural Bipartite Graph Matching</a></p><p>Kai Ye*, Siyan Dong*, Qingnan Fan, He Wang, <b>Li Yi</b>, Fei Xia, Jue Wang, Baoquan Chen</p><p>CVPR 2022</p><p><a href=\"https://arxiv.org/abs/2203.16319\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"189\" src=\"/__local/F/6F/E2/10E7B3CD784E811682BF2545209_A4CAB296_3DA05.png\" vsbhref=\"vurl\" vurl=\"/_vsl/F6FE210E7B3CD784E811682BF2545209/A4CAB296/3DA05\" vheight=\"189\" vwidth=\"333\" orisrc=\"/__local/F/6F/E2/10E7B3CD784E811682BF2545209_A4CAB296_3DA05.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">APES: Articulated Part Extraction from Sprite Sheets</a></p><p>Zhan Xu, Matthew Fisher, Yang Zhou, Deepali Aneja, Rushikesh Dudhat, <b>Li Yi</b>, Evangelos Kalogerakis</p><p>CVPR 2022</p><p><a href=\"https://ericyi.github.io/\">PDF</a> <a href=\"https://zhan-xu.github.io/parts\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"333\" height=\"189\" src=\"/__local/F/70/84/82477595B1053BE62058FE77939_00F9C38E_3DA05.png\" vsbhref=\"vurl\" vurl=\"/_vsl/F708482477595B1053BE62058FE77939/00F9C38E/3DA05\" vheight=\"189\" vwidth=\"333\" orisrc=\"/__local/F/70/84/82477595B1053BE62058FE77939_00F9C38E_3DA05.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Fixing Malfunctional Objects With Learned Physical Simulation and Functional Prediction</a></p><p>Yining Hong, Kaichun Mo, <b>Li Yi</b>, Leonidas J. Guibas, Antonio Torralba, Joshua B. Tenenbaum, Chuang Gan</p><p>CVPR 2022</p><p><a href=\"http://fixing-malfunctional.csail.mit.edu/assets/00889.pdf\">PDF</a> <a href=\"http://fixing-malfunctional.csail.mit.edu/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"212\" height=\"161\" src=\"/__local/6/34/B0/648BE7196BC7B8A0BDE7ECA2512_61722908_2171E.png\" vsbhref=\"vurl\" vurl=\"/_vsl/634B0648BE7196BC7B8A0BDE7ECA2512/61722908/2171E\" vheight=\"161\" vwidth=\"212\" orisrc=\"/__local/6/34/B0/648BE7196BC7B8A0BDE7ECA2512_61722908_2171E.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">PTR: A Benchmark for Part-based Conceptual, Relational, and Physical Reasoning</a></p><p>Yining Hong, <b>Li Yi</b>, Joshua B. Tenenbaum, Antonio Torralba, Chuang Gan</p><p>NeurIPS 2021</p><p><a href=\"https://arxiv.org/abs/2112.05136\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"358\" height=\"161\" src=\"/__local/3/AC/17/6CCE6CC46F977A189D58533FAEA_16449AD8_38705.png\" vsbhref=\"vurl\" vurl=\"/_vsl/3AC176CCE6CC46F977A189D58533FAEA/16449AD8/38705\" vheight=\"161\" vwidth=\"358\" orisrc=\"/__local/3/AC/17/6CCE6CC46F977A189D58533FAEA_16449AD8_38705.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Leveraging SE(3) Equivariance for Self-supervised Category-Level Object Pose Estimation from Point Clouds</a></p><p>Xiaolong Li, Yijia Weng, <b>Li Yi</b>, Leonidas J. Guibas, A. Lynn Abbott, Shuran Song, He Wang</p><p>NeurIPS 2021</p><p><a href=\"https://arxiv.org/abs/2111.00190\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"314\" height=\"161\" src=\"/__local/A/C2/93/94E67FDBBDD89B360F09B390667_4532D5E6_3181B.png\" vsbhref=\"vurl\" vurl=\"/_vsl/AC29394E67FDBBDD89B360F09B390667/4532D5E6/3181B\" vheight=\"161\" vwidth=\"314\" orisrc=\"/__local/A/C2/93/94E67FDBBDD89B360F09B390667_4532D5E6_3181B.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Contrastive Multimodal Fusion with TupleInfoNCE</a></p><p>Yunze Liu, Qingnan Fan, Shanghang Zhang, Hao Dong, Thomas Funkhouser, <b>Li Yi†</b></p><p>ICCV 2021</p><p><a href=\"https://arxiv.org/abs/2107.02575\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"178\" height=\"161\" src=\"/__local/F/46/9D/9CE224DD16328129CD25C02F1DA_F8DD1533_1C179.png\" vsbhref=\"vurl\" vurl=\"/_vsl/F469D9CE224DD16328129CD25C02F1DA/F8DD1533/1C179\" vheight=\"161\" vwidth=\"178\" orisrc=\"/__local/F/46/9D/9CE224DD16328129CD25C02F1DA_F8DD1533_1C179.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">P4Contrast: Contrastive Learning with Pairs of Point-Pixel Pairs for RGB-D Scene Understanding</a></p><p>Yunze Liu<sup>*</sup>, <b>Li Yi</b><sup>*</sup>, Shanghang Zhang, Qingnan Fan, Thomas Funkhouser, Hao Dong (* equal contribution)</p><p>arXiv:2012.13089 [cs.CV], Dec 2020</p><p><a href=\"https://arxiv.org/abs/2012.13089\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"286\" height=\"161\" src=\"/__local/D/EB/F2/6F53CC96BB3DEA0AE3CA2CE05C0_0CF390D0_2D193.png\" vsbhref=\"vurl\" vurl=\"/_vsl/DEBF26F53CC96BB3DEA0AE3CA2CE05C0/0CF390D0/2D193\" vheight=\"161\" vwidth=\"286\" orisrc=\"/__local/D/EB/F2/6F53CC96BB3DEA0AE3CA2CE05C0_0CF390D0_2D193.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Compositionally Generalizable 3D Structure Prediction</a></p><p>Songfang Han, Jiayuan Gu, Kaichun Mo, <b>Li Yi</b>, Siyu Hu, Xuejin Chen and Hao Su</p><p>arXiv:2012.02493 [cs.CV], Dec 2020</p><p><a href=\"https://arxiv.org/abs/2012.02493\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"211\" height=\"161\" src=\"/__local/3/8B/12/497E2B7102748EE7139321B02AB_94555514_2149A.png\" vsbhref=\"vurl\" vurl=\"/_vsl/38B12497E2B7102748EE7139321B02AB/94555514/2149A\" vheight=\"161\" vwidth=\"211\" orisrc=\"/__local/3/8B/12/497E2B7102748EE7139321B02AB_94555514_2149A.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments</a></p><p>Siyan Dong<sup>*</sup>, Qingnan Fan<sup>*</sup>, He Wang, Ji Shi, <b>Li Yi</b>, Thomas Funkhouser, Baoquan Chen, Leonidas J. Guibas (* equal contribution)</p><p>CVPR 2021 Oral Presentation</p><p><a href=\"https://arxiv.org/abs/2012.04746\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"305\" height=\"120\" src=\"/__local/4/66/5F/80AE860F4B19F586531CF5E608B_34557792_23D91.png\" vsbhref=\"vurl\" vurl=\"/_vsl/4665F80AE860F4B19F586531CF5E608B/34557792/23D91\" vheight=\"120\" vwidth=\"305\" orisrc=\"/__local/4/66/5F/80AE860F4B19F586531CF5E608B_34557792_23D91.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Complete &amp; Label: A Domain Adaptation Approach to Semantic Segmentation of LiDAR Point Clouds</a></p><p><b>Li Yi</b>, Boqing Gong, Thomas Funkhouser</p><p>CVPR 2021</p><p><a href=\"https://arxiv.org/abs/2007.08488\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"266\" height=\"161\" src=\"/__local/B/7C/37/FDC9CAC5450278D18DC74CC8A67_F0988E87_29F2B.png\" vsbhref=\"vurl\" vurl=\"/_vsl/B7C37FDC9CAC5450278D18DC74CC8A67/F0988E87/29F2B\" vheight=\"161\" vwidth=\"266\" orisrc=\"/__local/B/7C/37/FDC9CAC5450278D18DC74CC8A67_F0988E87_29F2B.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks</a></p><p>He Wang<sup>*</sup>, Zetian Jiang<sup>*</sup>, <b>Li Yi</b>, Kaichun Mo, Hao Su, Leonidas J. Guibas (* equal contribution)</p><p>CVPR 2021 Workshop on Learning to Generate 3D Shapes and Scenes</p><p><a href=\"https://arxiv.org/abs/2006.07029\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"314\" height=\"161\" src=\"/__local/C/B1/9C/C4D8B9B99B2F5841A4CEDE87221_E232FB1A_3181B.png\" vsbhref=\"vurl\" vurl=\"/_vsl/CB19CC4D8B9B99B2F5841A4CEDE87221/E232FB1A/3181B\" vheight=\"161\" vwidth=\"314\" orisrc=\"/__local/C/B1/9C/C4D8B9B99B2F5841A4CEDE87221_E232FB1A_3181B.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Curriculum DeepSDF</a></p><p>Yueqi Duan<sup>*</sup>, Haidong Zhu<sup>*</sup>, He Wang, <b>Li Yi</b>, Ram Nevatia, Leonidas J. Guibas (* equal contribution)</p><p>ECCV 2020</p><p><a href=\"https://arxiv.org/abs/2003.08593\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"211\" height=\"161\" src=\"/__local/A/87/E1/68FFFDCCAEAC7ACC14402583A80_823E2B7B_2149A.png\" vsbhref=\"vurl\" vurl=\"/_vsl/A87E168FFFDCCAEAC7ACC14402583A80/823E2B7B/2149A\" vheight=\"161\" vwidth=\"211\" orisrc=\"/__local/A/87/E1/68FFFDCCAEAC7ACC14402583A80_823E2B7B_2149A.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">SAPIEN: A SimulAted Part-based Interactive ENvironment</a></p><p>Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, <b>Li Yi</b>, Angel X.Chang, Leonidas J. Guibas, Hao Su</p><p>CVPR 2020 Oral Presentation</p><p><a href=\"https://arxiv.org/abs/2003.08515\">PDF</a> <a href=\"http://sapien.ucsd.edu/publication\">Project</a> <a href=\"https://github.com/haosulab/SAPIEN-Release\">Code</a> <a href=\"https://youtu.be/K2yOeJhJXzM\">Demo</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"218\" height=\"161\" src=\"/__local/E/DC/74/2311FBA4A27EFBC0371CD980ADA_49BFBD72_22642.png\" vsbhref=\"vurl\" vurl=\"/_vsl/EDC742311FBA4A27EFBC0371CD980ADA/49BFBD72/22642\" vheight=\"161\" vwidth=\"218\" orisrc=\"/__local/E/DC/74/2311FBA4A27EFBC0371CD980ADA_49BFBD72_22642.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Category-Level Articulated Object Pose Estimation</a></p><p>Xiaolong Li<sup>*</sup>, He Wang<sup>*</sup>, <b>Li Yi</b>, Leonidas Guibas, A. Lynn Abbott, Shuran Song (* equal contribution)</p><p>CVPR 2020 Oral Presentation</p><p><a href=\"https://articulated-pose.github.io/paper.pdf\">PDF</a> <a href=\"https://articulated-pose.github.io/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"250\" height=\"161\" src=\"/__local/6/E1/AF/7D196A33FA9B2020D6926243FFB_ED34BCBE_276DA.png\" vsbhref=\"vurl\" vurl=\"/_vsl/6E1AF7D196A33FA9B2020D6926243FFB/ED34BCBE/276DA\" vheight=\"161\" vwidth=\"250\" orisrc=\"/__local/6/E1/AF/7D196A33FA9B2020D6926243FFB_ED34BCBE_276DA.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">StructEdit: Learning Structural Shape Variations</a></p><p>Kaichun Mo<sup>*</sup>, Paul Guerrero<sup>*</sup>, <b>Li Yi</b>, Hao Su, Peter Wonka, Niloy Mitra, Leonidas J. Guibas (* equal contribution)</p><p>CVPR 2020</p><p><a href=\"https://arxiv.org/abs/1911.11098\">PDF</a> <a href=\"https://cs.stanford.edu/~kaichun/structedit/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"281\" height=\"134\" src=\"/__local/A/17/0B/9F679C3FE6EB5BA238E3CBDDD28_86393110_24E23.png\" vsbhref=\"vurl\" vurl=\"/_vsl/A170B9F679C3FE6EB5BA238E3CBDDD28/86393110/24E23\" vheight=\"134\" vwidth=\"281\" orisrc=\"/__local/A/17/0B/9F679C3FE6EB5BA238E3CBDDD28_86393110_24E23.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">AdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency Loss</a></p><p>Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, <b>Li Yi</b>, Leonidas J. Guibas, Hao Zhang</p><p>CVPR 2020 Oral Presentation</p><p><a href=\"https://arxiv.org/abs/1903.10297\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"223\" height=\"161\" src=\"/__local/3/96/9D/4F7F898EE7533D34C4C5FFBA9B8_131BDC21_232D6.png\" vsbhref=\"vurl\" vurl=\"/_vsl/3969D4F7F898EE7533D34C4C5FFBA9B8/131BDC21/232D6\" vheight=\"161\" vwidth=\"223\" orisrc=\"/__local/3/96/9D/4F7F898EE7533D34C4C5FFBA9B8_131BDC21_232D6.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">StructureNet: Hierarchical Graph Networks for 3D Shape Generation</a></p><p>Kaichun Mo<sup>*</sup>, Paul Guerrero<sup>*</sup>, <b>Li Yi</b>, Hao Su, Peter Wonka, Niloy Mitra, Leonidas J. Guibas (* equal contribution)</p><p>SIGGRAPH Asia 2019</p><p><a href=\"https://arxiv.org/abs/1908.00575\">PDF</a> <a href=\"https://cs.stanford.edu/~kaichun/structurenet/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"230\" height=\"161\" src=\"/__local/4/DC/25/676B1BFE38A93955CC1FA1425EC_ADB8A058_2447E.png\" vsbhref=\"vurl\" vurl=\"/_vsl/4DC25676B1BFE38A93955CC1FA1425EC/ADB8A058/2447E\" vheight=\"161\" vwidth=\"230\" orisrc=\"/__local/4/DC/25/676B1BFE38A93955CC1FA1425EC_ADB8A058_2447E.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud</a></p><p><b>Li Yi</b>, Wang Zhao, He Wang, Minhyuk Sung, Leonidas Guibas</p><p>CVPR 2019</p><p><a href=\"https://arxiv.org/abs/1812.03320\">PDF</a> <a href=\"https://github.com/ericyi/GSPN\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"248\" height=\"107\" src=\"/__local/1/3C/96/D57C454F9D44EF1D6ACAE8D3AD6_57F522AA_1A003.png\" vsbhref=\"vurl\" vurl=\"/_vsl/13C96D57C454F9D44EF1D6ACAE8D3AD6/57F522AA/1A003\" vheight=\"107\" vwidth=\"248\" orisrc=\"/__local/1/3C/96/D57C454F9D44EF1D6ACAE8D3AD6_57F522AA_1A003.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">TextureNet: Consistent Local Parametrizations for Learning from High-Resolution Signals on Meshes</a></p><p>Jingwei Huang, Haotian Zhang, <b>Li Yi</b>, Thomas Funkhouser, Matthias Niessner, Leonidas Guibas</p><p>CVPR 2019 Oral Presentation</p><p><a href=\"https://arxiv.org/abs/1812.00020\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"236\" height=\"134\" src=\"/__local/6/9B/EA/5071334184114F3E77E8AF1FEF2_7006FD67_1EFC2.png\" vsbhref=\"vurl\" vurl=\"/_vsl/69BEA5071334184114F3E77E8AF1FEF2/7006FD67/1EFC2\" vheight=\"134\" vwidth=\"236\" orisrc=\"/__local/6/9B/EA/5071334184114F3E77E8AF1FEF2_7006FD67_1EFC2.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Supervised Fitting of Geometric Primitives to 3D Point Clouds</a></p><p>Lingxiao Li<sup>*</sup>, Minhyuk Sung<sup>*</sup>, Anastasia Dubrovina, <b>Li Yi</b>, Leonidas Guibas (* equal contribution)</p><p>CVPR 2019 Oral Presentation</p><p><a href=\"https://arxiv.org/abs/1811.08988\">PDF</a> <a href=\"https://github.com/csimstu2/SPFN\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"182\" height=\"187\" src=\"/__local/C/82/53/153373DC8B47A2CF5AC52006CAC_F93BBC66_215B0.png\" vsbhref=\"vurl\" vurl=\"/_vsl/C8253153373DC8B47A2CF5AC52006CAC/F93BBC66/215B0\" vheight=\"187\" vwidth=\"182\" orisrc=\"/__local/C/82/53/153373DC8B47A2CF5AC52006CAC_F93BBC66_215B0.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding</a></p><p>Kaichun Mo, Shilin Zhu, Angel X.Chang, <b>Li Yi</b>, Subarna Tripathi, Leonidas J. Guibas and Hao Su</p><p>CVPR 2019</p><p><a href=\"https://arxiv.org/abs/1812.02713\">PDF</a> <a href=\"https://cs.stanford.edu/~kaichun/partnet/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"199\" height=\"134\" src=\"/__local/8/A7/FF/557E9C973144D3BE5A28E732CB8_112D967A_1A232.png\" vsbhref=\"vurl\" vurl=\"/_vsl/8A7FF557E9C973144D3BE5A28E732CB8/112D967A/1A232\" vheight=\"134\" vwidth=\"199\" orisrc=\"/__local/8/A7/FF/557E9C973144D3BE5A28E732CB8_112D967A_1A232.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">GeoNet: Deep Geodesic Networks for Point Cloud Analysis</a></p><p>Tong He, Haibin Huang, <b>Li Yi</b>, Yuqian Zhou, Qihao Wu, Jue Wang, Stefano Soatto</p><p>CVPR 2019 Oral Presentation</p><p><a href=\"https://arxiv.org/abs/1901.00680\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"200\" height=\"161\" src=\"/__local/9/4B/56/740D4C974EBD8EE571DE305FF77_C6E36E22_1F8DD.png\" vsbhref=\"vurl\" vurl=\"/_vsl/94B56740D4C974EBD8EE571DE305FF77/C6E36E22/1F8DD\" vheight=\"161\" vwidth=\"200\" orisrc=\"/__local/9/4B/56/740D4C974EBD8EE571DE305FF77_C6E36E22_1F8DD.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Deep Part Induction from Articulated Object Pairs</a></p><p><b>Li Yi</b>, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su, Leonidas Guibas</p><p>SIGGRAPH Asia 2018</p><p><a href=\"https://arxiv.org/pdf/1809.07417.pdf\">PDF</a> <a href=\"https://github.com/ericyi/articulated-part-induction\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"161\" height=\"161\" src=\"/__local/4/81/19/FCAB117B6F7996F1E614787C225_C85DE0DC_1969D.png\" vsbhref=\"vurl\" vurl=\"/_vsl/48119FCAB117B6F7996F1E614787C225/C85DE0DC/1969D\" vheight=\"161\" vwidth=\"161\" orisrc=\"/__local/4/81/19/FCAB117B6F7996F1E614787C225_C85DE0DC_1969D.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Beyond Holistic Object Recognition: Enriching Image Understanding with Part States</a></p><p>Cewu Lu, Hao Su, Yongyi Lu, <b>Li Yi</b>, Chikeung Tang, Leonidas Guibas</p><p>CVPR 2018</p><p><a href=\"https://arxiv.org/pdf/1612.07310.pdf\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"281\" height=\"161\" src=\"/__local/D/FA/16/5345841B691CF518B02F210DBA2_B52FF31A_2C4FF.png\" vsbhref=\"vurl\" vurl=\"/_vsl/DFA165345841B691CF518B02F210DBA2/B52FF31A/2C4FF\" vheight=\"161\" vwidth=\"281\" orisrc=\"/__local/D/FA/16/5345841B691CF518B02F210DBA2_B52FF31A_2C4FF.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a></p><p>Charles R. Qi, <b>Li Yi</b>, Hao Su, Leonidas J. Guibas</p><p>NIPS 2017</p><p><a href=\"https://arxiv.org/pdf/1706.02413\">PDF</a> <a href=\"http://stanford.edu/~rqi/pointnet2/\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"214\" height=\"161\" src=\"/__local/2/6D/ED/CCC66FC9B03406D171C4479FA7D_CE6A60F6_21C26.png\" vsbhref=\"vurl\" vurl=\"/_vsl/26DEDCCC66FC9B03406D171C4479FA7D/CE6A60F6/21C26\" vheight=\"161\" vwidth=\"214\" orisrc=\"/__local/2/6D/ED/CCC66FC9B03406D171C4479FA7D_CE6A60F6_21C26.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">Learning Hierarchical Shape Segmentation and Labeling from Online Repositories</a></p><p><b>Li Yi</b>, Leonidas J. Guibas, Aaron Hertzmann, Vladimir G. Kim, Hao Su, Ersin Yumer</p><p>SIGGRAPH 2017</p><p><a href=\"https://arxiv.org/pdf/1705.01661.pdf\">PDF</a> <a href=\"http://cs.stanford.edu/~ericyi/project_page/hier_seg/index.html\">Project</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"255\" height=\"161\" src=\"/__local/5/EC/4B/334459DB282FA9EADB68A222550_469700C0_2837A.png\" vsbhref=\"vurl\" vurl=\"/_vsl/5EC4B334459DB282FA9EADB68A222550/469700C0/2837A\" vheight=\"161\" vwidth=\"255\" orisrc=\"/__local/5/EC/4B/334459DB282FA9EADB68A222550_469700C0_2837A.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation</a></p><p><b>Li Yi</b>, Hao Su, Xingwen Guo, Leonidas J. Guibas</p><p>CVPR 2017 Spotlight Presentation</p><p><a href=\"https://arxiv.org/pdf/1612.00606v1.pdf\">PDF</a> <a href=\"https://github.com/ericyi/SyncSpecCNN\">Code</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"204\" height=\"161\" src=\"/__local/F/B1/0B/9C4A3F0B66177ECFF4EF6E87002_03159848_202F9.png\" vsbhref=\"vurl\" vurl=\"/_vsl/FB10B9C4A3F0B66177ECFF4EF6E87002/03159848/202F9\" vheight=\"161\" vwidth=\"204\" orisrc=\"/__local/F/B1/0B/9C4A3F0B66177ECFF4EF6E87002_03159848_202F9.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"https://ericyi.github.io/\">A Scalable Active Framework for Region Annotation in 3D Shape Collections</a></p><p><b>Li Yi</b>, Vladimir G. Kim, Duygu Ceylan, I-Chao Shen, Mengyuan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, Leonidas J. Guibas</p><p>SIGGRAPH Asia 2016</p><p><a href=\"http://cs.stanford.edu/~ericyi/papers/part_annotation_16.pdf\">PDF(23.2MB)</a> <a href=\"http://cs.stanford.edu/~ericyi/papers/part_annotation_16_small.pdf\">PDF(6.8MB)</a> <a href=\"http://cs.stanford.edu/~ericyi/papers/part_annotation_16_supplemental.pdf\">Supplemental</a> <a href=\"http://cs.stanford.edu/~ericyi/project_page/part_annotation/index.html\">Project</a> <a href=\"http://cs.stanford.edu/~ericyi/project_page/part_annotation/bib.html\">Bib</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"161\" height=\"161\" src=\"/__local/7/14/B1/7BF1B95A4397DBBC4D36A9B6AE0_332F1B62_1969D.png\" vsbhref=\"vurl\" vurl=\"/_vsl/714B17BF1B95A4397DBBC4D36A9B6AE0/332F1B62/1969D\" vheight=\"161\" vwidth=\"161\" orisrc=\"/__local/7/14/B1/7BF1B95A4397DBBC4D36A9B6AE0_332F1B62_1969D.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"http://arxiv.org/abs/1512.03012\">ShapeNet: An Information-Rich 3D Model Repository</a></p><p>Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, <b>Li Yi</b>, and Fisher Yu</p><p>arXiv:1512.03012 [cs.GR], Dec 2015</p><p><a href=\"http://arxiv.org/abs/1512.03012\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"153\" height=\"161\" src=\"/__local/6/C9/F3/B7A93461A213EF228AD814CF6A1_F5637F5F_18278.png\" vsbhref=\"vurl\" vurl=\"/_vsl/6C9F3B7A93461A213EF228AD814CF6A1/F5637F5F/18278\" vheight=\"161\" vwidth=\"153\" orisrc=\"/__local/6/C9/F3/B7A93461A213EF228AD814CF6A1_F5637F5F_18278.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"http://geometry.stanford.edu/papers/swyg-3afsnvo-15/swyg-3afsnvo-15.pdf\">3D-Assisted Image Feature Synthesis for Novel Views of an Object</a></p><p>Hao Su<sup>*</sup>, Fan Wang<sup>*</sup>, <b>Li Yi</b>, Leonidas Guibas</p><p>ICCV 2015 oral</p><p><a href=\"http://geometry.stanford.edu/papers/swyg-3afsnvo-15/swyg-3afsnvo-15.pdf\">PDF</a></p></td>\n  </tr>\n  <tr>\n   <td width=\"358\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p class=\"imgh\"><img width=\"161\" height=\"161\" src=\"/__local/8/64/D1/3CDF9CA18D5737A5D440CC23B03_631A6005_1969D.png\" vsbhref=\"vurl\" vurl=\"/_vsl/864D13CDF9CA18D5737A5D440CC23B03/631A6005/1969D\" vheight=\"161\" vwidth=\"161\" orisrc=\"/__local/8/64/D1/3CDF9CA18D5737A5D440CC23B03_631A6005_1969D.png\" class=\"img_vsb_content\"></p></td>\n   <td width=\"778\" style=\"border-color: rgb(221, 221, 221); border-style: solid; border-width: 1px; padding: 5px 10px;\"><p><a href=\"http://cs.stanford.edu/~ericyi/papers/super_resolution_13.pdf\">Image Super-Resolution Via Analysis Sparse Prior</a></p><p>Qiang Ning, Kan Chen, <b>Li Yi</b>, Chuchu Fan, Yao Lu, Jiangtao Wen</p><p>IEEE Signal Processing Letter 2013</p><p><a href=\"http://cs.stanford.edu/~ericyi/papers/super_resolution_13.pdf\">PDF</a></p></td>\n  </tr>\n </tbody>\n</table></div></div>\n \n  </div>\n  \n</div>\n\n          <div class=\"box box0 box5 aos-init\" data-aos=\"fade-up\">\n            <h3 class=\"h3-5 flex\"><img src=\"../../images/h3-5.png\" alt=\"\">相关资讯</h3>\n            \n\n\n\n\n\n\n\n<ul class=\"ls33 flexjs\">\n</ul>\n\n          </div>\n        </div>\n        <div class=\"right\">\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<script language=\"javascript\" src=\"/system/resource/js/ajax.js\"></script><script language=\"javascript\">_getBatchClickTimes('null',2110873241,'wbnews','u12')</script>\n<script>function seeContenta12(contentid,size,displayid){    document.getElementById(contentid).innerHTML = '[';    for(var i=0;i<=size;i++){        var allcontentid = contentid+(i+1);        if(allcontentid==displayid){            document.getElementById(contentid).innerHTML += \" <span id='\"+allcontentid+\"' name='\"+allcontentid+\"'  >\"+(i+1)+\"</span> \";            document.getElementById(displayid).style.display = 'block';        }else{            document.getElementById(contentid).innerHTML += \" <span style='cursor:pointer' id='\"+allcontentid+\"' name='\"+allcontentid+\"' onclick=seeContenta12('\"+contentid+\"','\"+size+\"','\"+allcontentid+\"')  >\"+(i+1)+\"</span> \";            document.getElementById(allcontentid).style.display = 'none';        }    }    document.getElementById(contentid).innerHTML += ']';}</script>\n\n<script>_addDynClicks('wbnews',2110873241,3936)</script>\n<div class=\"sticky\">\n            <div class=\"box\">\n      \n              <div class=\"con\">\n                <h4>Email</h4>\n                <h6><img src=\"../../zpyx/27str2img.png\" alt=\"\"></h6>\n              </div>\n\n              \n              <div class=\"con\" style=\"display:none;\"></div>\n              <div class=\"con\" style=\"display:none;\"></div>\n              <div class=\"con\">\n                <h4>Google Scholar</h4>\n                <h6>https://scholar.google.com/citations?user=UyZL660AAAAJ&amp;hl=en</h6>\n              </div>\n              \n              <div class=\"con\" style=\"display:none;\"></div>\n              \n              <div class=\"con\" style=\"display:none;\"></div>\n            </div>\n          </div>\n</div>\n      </div>\n    </div>\n  </div>\n\n   <div class=\"footer section fp-auto-height\">\n          <div class=\"wp\"><!-- 版权内容请在本组件\"内容配置-版权\"处填写 -->\n<p>地址：北京市 海淀区 清华大学 信息科学技术楼(FIT楼) 1-208室 100084 电话：010-62781693</p><p>传真： 010-62781693-转2000 邮箱：iiis@mail.tsinghua.edu.cn 版权所有 @ 清华大学交叉信息研究院</p></div>\n        </div>\n</div>\n\n<div class=\"ser-layer\">\n      <span class=\"serclose swi-close-outlined\"></span>\n      <div class=\"serform\"><script type=\"text/javascript\">\n    function _nl_ys_check(){\n        \n        var keyword = document.getElementById('showkeycode1161673').value;\n        if(keyword==null||keyword==\"\"){\n            alert(\"请输入你要检索的内容！\");\n            return false;\n        }\n        if(window.toFF==1)\n        {\n            document.getElementById(\"lucenenewssearchkey1161673\").value = Simplized(keyword );\n        }else\n        {\n            document.getElementById(\"lucenenewssearchkey1161673\").value = keyword;            \n        }\n        var  base64 = new Base64();\n        document.getElementById(\"lucenenewssearchkey1161673\").value = base64.encode(document.getElementById(\"lucenenewssearchkey1161673\").value);\n        new VsbFormFunc().disableAutoEnable(document.getElementById(\"showkeycode1161673\"));\n        return true;\n    } \n</script>\n<form action=\"../../ssjg.jsp?wbtreeid=1075\" method=\"post\" id=\"au14a\" name=\"au14a\" onsubmit=\"return _nl_ys_check()\" style=\"display: inline\">\n <input type=\"hidden\" id=\"lucenenewssearchkey1161673\" name=\"lucenenewssearchkey\" value=\"\"><input type=\"hidden\" id=\"_lucenesearchtype1161673\" name=\"_lucenesearchtype\" value=\"1\"><input type=\"hidden\" id=\"searchScope1161673\" name=\"searchScope\" value=\"0\">\n\n     <div class=\"input-group pore\">\n          <input type=\"text\" name=\"showkeycode\" id=\"showkeycode1161673\" class=\"inp\" placeholder=\"输入关键词搜索...\" autocomplete=\"off\">\n          <button class=\"sub\" type=\"submit\" align=\"absmiddle\" style=\"cursor: hand\"></button>\n     </div>\n \n</form><script language=\"javascript\" src=\"/system/resource/js/base64.js\"></script><script language=\"javascript\" src=\"/system/resource/js/formfunc.js\"></script>\n</div>\n    </div>\n\n<div class=\"hide\" id=\"gotop\">TOP</div>\n\n<script src=\"../../js/public.js\"></script>\n<script src=\"../../js/wave.js\"></script>\n<style>\n.n_pad1{ overflow: visible;}\n</style>\n\n\n\n</body></html>"
}