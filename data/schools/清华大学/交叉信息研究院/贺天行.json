{
    "html": "<!DOCTYPE html><html lang=\"en\"><head><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n\n  <title>Tianxing He </title>\n  \n  <meta name=\"author\" content=\"Tianxing He\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n  \n  <link rel=\"stylesheet\" type=\"text/css\" href=\"stylesheet.css\">\n  <link rel=\"icon\" type=\"image/png\" href=\"images/head_icon_htx.png\"> \n</head>\n\n<body>\n  <table style=\"width:160%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;\"><tbody>\n    <tr style=\"padding:0px\">\n      <td style=\"padding:0px\">\n        <table style=\"width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;\"><tbody>\n          <tr style=\"padding:0px\">\n            <td style=\"padding:2.5%;width:63%;vertical-align:middle\">\n              <p style=\"text-align:center\">\n                <name>Tianxing He</name>\n              </p>\n              I have joined THU-IIIS (also known as the Yao Class) as AP in Sep 2024. I also work part-time at Shanghai Qi Zhi Institute. I will teach a undergraduate NLP course in Fall 2025.<br>\n\t      My research focus on AI safety and AI-driven simulation. <br>\n\t      <br> Below are some recruitment calls in Chinese. <br>\n\t      <a href=\"https://zhuanlan.zhihu.com/p/1891963811541521383\">(closed) 2025 IIIS PhD student recruitment (2026 entrance)</a>. <br>\n              <a href=\"https://zhuanlan.zhihu.com/p/716401569\">Full-time post-bachelor position at Shanghai Qi Zhi Institute</a>, for research-oriented students at gap year. <br>\n\t      <a href=\"https://zhuanlan.zhihu.com/p/1909656383441310317\">Video game development engineer position at Shanghai Qi Zhi Institute</a>, for students or engineers who are interested in the intersection of gaming and AI. <br>\n\t      <!-- <p> 目前我想招几个research intern，做LLM社会模拟方向（如AI小镇），或者AI+游戏方向（比如minecraft agent或者游戏关卡生成），以及AI安全方向。需要有很强的自学能力和对科研项目的极大热情。我希望你来主导项目而我主要起到讨论和资源支持的作用。有兴趣的同学请email联系我，直接中文就行。（一些paper例子：https://arxiv.org/abs/2302.05981，https://arxiv.org/pdf/2310.03903，https://arxiv.org/pdf/2403.19267，https://arxiv.org/pdf/2106.10155，https://arxiv.org/abs/2304.03442，https://arxiv.org/pdf/2408.00989）</p> -->\n\t      <!-- <p> 关于研究生（硕士博士）的招生，我们只通过每年的夏令营招生，今年的夏令营已经结束了，我现在只能招实习生。</p> -->\n              <p> In 2024 I was a postdoc at UW, supervised by Yulia Tsvetkov, who runs the <a href=\"https://tsvetshop.github.io/#people\">Tsvetshop</a>.\n\t      A while ago, I was a PhD student at MIT, supervised by Prof. James Glass, who runs the <a href=\"http://groups.csail.mit.edu/sls/\">SLS group</a>.  <br>\n              \n              \n                I did my bachelor and master degree at Shanghai Jiao Tong University, and my research there was supervised by Prof. Kai Yu, who runs the <a href=\"https://speechlab.sjtu.edu.cn/\"> SJTU SpeechLab</a>.\n                At SJTU I was in the ACM honored class.\n              </p>\n \n              \n\t      \n              <!--\n              <p> \n\t      <b> Research mentorship with self-motivated undergrad students: </b> For the year 2024, I am currently interested in exploring two directions: (1) AI safety (e.g., red teaming or novel algorithms); (2) The connection between AI and video games (e.g., Minecraft) or game engines such as Unity. If you are interested and can work with me for at least 5 months, shoot me an email. I have maintained a very healthy and fruitful record with undergrad students. You can refer to my publications below (the ones with me as the last author or co-first author) or <a href='slides/teaching_statement.pdf'>my teaching statement</a>.\n              </p>\n              -->\n\n     \n             <p style=\"text-align:center\">\n                <a href=\"mailto:hetianxing@mail.tsinghua.edu.cn\">Email</a> &nbsp;/&nbsp;\n                <a href=\"https://scholar.google.com/citations?user=egmfjjwAAAAJ&amp;hl=en\">Google Scholar</a> &nbsp;/&nbsp;\n                <a href=\"https://twitter.com/TianxingH\">Twitter</a> \n              </p>\n            </td>\n            <td style=\"padding:2.5%;width:40%;max-width:40%\">\n              <a href=\"images/photo_twodog.png\"><img style=\"width:100%;max-width:100%\" alt=\"profile photo\" src=\"images/photo_twodog.png\" class=\"hoverZoomLink\"></a>\n            </td>\n          </tr>\n        </tbody></table>\n        <table style=\"width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;\"><tbody>\n            <tr>\n            <td style=\"padding:20px;width:100%;vertical-align:middle\">\n              <heading>Research Projects</heading>\n              <p>\n                Listed below. * means co-first-author.\n              </p>\n            </td>\n          </tr>\n        </tbody></table>\n        <table style=\"width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;\"><tbody>\n\n\t  <tr bgcolor=\"#ffffd0\">\n            <td style=\"padding:40px;width:25%;vertical-align:middle\">\n                  <img src=\"images/aicrypto.png\" width=\"220\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2507.09580\">\n                <papertitle>AICrypto: A Comprehensive Benchmark For Evaluating Cryptography Capabilities of Large Language Models</papertitle>\n              </a>\n              <br>\n              Yu Wang*, Yijian Liu*, Liheng Ji*, Han Luo*, Wenjie Li*, Xiaofei Zhou, Chiyun Feng, Puji Wang, Yuhan Cao, Geyuan Zhang, Xiaojian Li, Rongwu Xu, Yilei Chen, Tianxing He\n              <br>\n\t\t\t\t\t\t\t<em>arXiv</em>\n              <br>\n              <p></p>\n              <p>\n\t      We evaluate and show that LLM is pretty good at undergraduate-level crypto. The benchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF) challenges, and 18 proof problems, covering a broad range of skills from factual memorization to vulnerability exploitation and formal reasoning. All tasks are carefully reviewed or constructed by cryptography experts (huge thanks to Prof.Chen) to ensure correctness and rigor. \n\t      </p>\n            </td>\n         </tr> \n\n\t <tr>\n            <td style=\"padding:20px 60px;width:25%;vertical-align:middle\">\n                  <img src=\"images/iiis_aisafety_lunch.png\" width=\"120\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n                <papertitle>IIIS AI Safety Lunch</papertitle>\n              <br>\n              <br>\n              <p></p>\n              <p>\n              We do a monthly lunch with the topic of AI safety, at the FIT Building, Tsinghua, Beijing. The first one in the fall semester will be at Sep 05, in Chinese language. Two students will present in each lunch. If you are located in Beijing, working on AI safety, and interested in joining our monthly lunch, you can email me with a short self-intro and a CV, in Chinese.\n\t      </p>\n            </td>\n          </tr> \n\n\n\t <tr>\n            <td style=\"padding:20px 60px;width:25%;vertical-align:middle\">\n                  <img src=\"images/2025_anime_roleplay.png\" width=\"150\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2508.06388\">\n                <papertitle>LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing</papertitle>\n              </a>\n              <br>\n              Lanlan Qiu, Xiao Pu, Yeqi Feng, Tianxing He\n              <br>\n              <p></p>\n              <p>\n              We introduce ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We first thoughtfully select 20 top-tier characters from popular anime communities and design 60 emotion-centric real-world scenario questions. Then, we execute a nationwide selection process to identify 40 Chinese anime enthusiasts. Next, we systematically collect two rounds of dialogue data from 10 LLMs and the human fans. Experimental results show that top-performing LLMs surpass human fans in role-playing and emotional support, while humans still lead in response diversity.\n              </p>\n            </td>\n          </tr> \n\n\n\t<tr bgcolor=\"#ffffd0\">\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/miniagentpro.png\" width=\"250\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://github.com/Just-A-Pie/MiniAgentStudio/\">\n                <papertitle>MiniAgents-Pro: A New Visualization Interface for Generative Agents (AI Town)</papertitle>\n              </a>\n              <br>\n              Yuyang Tian* (back end), Shunqiang Mao* (front end), Wenchang Gao, Tianxing He\n              <br>\n              <p></p>\n              <p>\n\t      We use the Unity game engine to build an improved visualization interface (frontend) for generative agents. It visualizes trajectory data generated by a backend LLM simulation. Major features: map editor, items, short-time summary (provided by backend), easy asset adding, and other basic features. We also open-source our Unity <a href=\"https://github.com/Just-A-Pie/MiniAgentPro\">code</a>.\n              </p>\n            </td>\n          </tr> \n\n\n\t <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/tcgbench.png\" width=\"220\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2506.06821\">\n                <papertitle>Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems</papertitle>\n              </a>\n              <br>\n              Yuhan Cao*, Zian Chen*, Kun Quan, Ziliang Zhang, Yu Wang, Xiaoning Dong, Yeqi Feng, Guanzhong He, Jingcheng Huang, Jianhao Li, Yixuan Tan, Jiafu Tang, Yilin Tang, Junlei Wu, Qianyu Xiao, Can Zheng, Shouchen Zhou, Yuxiang Zhu, Yiming Huang, Tian Xie, Tianxing He\n              <br>\n\t\t\t\t\t\t\t<em>arXiv</em>\n              <br>\n              <p></p>\n              <p>\n\t      We construct TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This benchmark comprises two tasks, aimed at studying the capabilities of LLMs in (1) generating valid test case generators for a given CP problem, and further (2) generating targeted test case generators that expose bugs in human-written code. Experimental results indicate that while state-of-the-art LLMs can generate valid test case generators in most cases, most LLMs struggle to generate targeted test cases that reveal flaws in human code effectively. \n\t      </p>\n            </td>\n         </tr> \n\n\n\t <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/integrityattack.png\" width=\"220\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://www.arxiv.org/abs/2506.04572\">\n                <papertitle>Demonstrations of Integrity Attacks in Multi-Agent Systems</papertitle>\n              </a>\n              <br>\n              Can Zheng, Yuhan Cao, Xiaoning Dong, Tianxing He\n              <br>\n\t\t\t\t\t\t\t<em>arXiv</em>\n              <br>\n              <p></p>\n              <p>\n\t     MAS could be vulnerable to malicious agents that exploit the system to serve self-interests without disrupting its core functionality. This work explores integrity attacks where malicious agents employ subtle prompt manipulation to bias MAS operations and gain various benefits. Four types of attacks are examined: Scapegoater, Boaster, Self-Dealer, and Free-Rider.\n\t      </p>\n            </td>\n         </tr> \n\n\n\n\t  <tr bgcolor=\"#ffffd0\">\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/2025_vacsim.png\" width=\"220\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/pdf/2503.09639\">\n                <papertitle>Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy</papertitle>\n              </a>\n              <br>\n              Abe Bohan Hou, Hongru Du, Yichen Wang, Jingyu Zhang, Zixiao Wang, Paul Pu Liang, Daniel Khashabi, Lauren Gardner, Tianxing He\n              <br>\n\t\t\t\t\t\t\t<em>COLM 2025</em>\n              <br>\n              <p></p>\n              <p>\n              We introduce the VACSIM framework with 100 generative agents powered by Large Language Models (LLMs). VACSIM simulates vaccine policy outcomes with the following steps: 1) Instantiate a population of agents with demographics based on census data; 2) Connect the agents via a social network and model vaccine attitudes as a function of social dynamics and disease-related information; 3) Design and evaluate various public health interventions aimed at mitigating vaccine hesitancy.\n\t      </p>\n            </td>\n         </tr> \n\n\n\t  <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/acl2025_mml.png\" width=\"220\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/pdf/2412.00473\">\n                <papertitle>Jailbreak Large Vision-Language Models Through Multi-Modal Linkage</papertitle>\n              </a>\n              <br>\n              Yu Wang, Xiaofei Zhou, Yichen Wang, Geyuan Zhang, Tianxing He\n              <br>\n\t\t\t\t\t\t\t<em>ACL 2025</em>\n              <br>\n              <p></p>\n              <p>\n                Drawing inspiration from cryptography, our jailbreak utilizes an encryption-decryption process across text and image modalities to mitigate over-exposure of malicious information. To align the model's output with malicious intent covertly, we also employ a technique called \"evil alignment\", framing the attack within a video game production scenario.\n              </p>\n            </td>\n         </tr> \n\n\n\t<tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/2024_mia.jpg\" width=\"220\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/pdf/2405.20771\">\n                <papertitle>Towards Black-Box Membership Inference Attack for Diffusion Models</papertitle>\n              </a>\n              <br>\n              Jingwei Li, Jing Dong, Tianxing He, Jingzhao Zhang\n              <br>\n\t\t\t\t\t\t\t<em>ICML 2025</em>\n              <br>\n              <p></p>\n              <p>\n              We propose a very simple and effective black-box MIA (membership inference attack) algorithm for diffusion models.\n              </p>\n            </td>\n         </tr> \n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/2024_hiergen.jpg\" width=\"220\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2404.16367\">\n                <papertitle>Learning Syntax without Planting Trees: Understanding When and  Why Transformers Generalize Hierarchically</papertitle>\n              </a>\n              <br>\n              Kabir Ahuja, Vidhisha Balachandran, Madhur Panwar, Tianxing He, Noah A.Smith, Navin Goyal, Yulia Tsvetkov\n              <br>\n\t\t\t\t\t\t\t<em>TACL 2025</em>\n              <br>\n              <p></p>\n              <p>\n              We extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g., sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically.  \n              </p>\n            </td>\n         </tr> \n\n\n         <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/2024_miniagents.jpg\" width=\"250\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://github.com/cloudygoose/MiniAgents\">\n                <papertitle>MiniAgents: A Visualization Interface for Simulacra</papertitle>\n              </a>\n              <br>\n              Tianxing He\n              <br>\n              <p></p>\n              <p>\n              I tried to using the Unity game engine to build a visualization interface for simulacra. It's now functional and you are welcome to try it. I have stopped working on it myself, but some of my students are working on a better version of it.\n              </p>\n            </td>\n          </tr> \n\n\n         <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/2024_attackdetect.jpg\" width=\"220\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2402.11638\">\n                <papertitle>Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks</papertitle>\n              </a>\n              <br>\n              Yichen Wang, Shangbin Feng, Abe Bohan Hou, Xiao Pu, Chao Shen, Xiaoming Liu, Yulia Tsvetkov, Tianxing He\n              <br>\n\t\t\t\t\t\t\t<em>ACL 2024</em>\n              <br>\n              <p></p>\n              <p>\n              We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, prompting, and co-generating. \n              </p>\n            </td>\n          </tr> \n\n        \n         <tr bgcolor=\"#ffffd0\">\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/blindspot.png\" width=\"220\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2212.10020\">\n                <papertitle>On the Blind Spots of Model-Based Evaluation Metrics for Text Generation</papertitle>\n              </a>\n              <br>\n              Tianxing He*, Jingyu Zhang*, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass, Yulia Tsvetkov\n\n              <br>\n\t\t\t\t\t\t\t<em>ACL 2023</em>, <a href=\"slides/2023_12mintalk_blindspot.pdf\">selfcontained-oral-slide</a>\n              <br>\n              <p></p>\n              <p>\n              In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation.\n              </p>\n            </td>\n          </tr> \n\n\n            <tr bgcolor=\"#ffffd0\">\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/semstamp.jpg\" width=\"220\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2310.03991\">\n                <papertitle>SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation</papertitle>\n              </a>\n              <br>\n              Abe Bohan Hou*, Jingyu Zhang*, Tianxing He*, Yichen Wang, Yung-Sung Chuang, Hongwei Wang, Lingfeng Shen, Benjamin Van Durme, Daniel Khashabi, Yulia Tsvetkov\n              <br>\n\t      <em>NAACL 2024</em>\n              <br>\n              <p></p>\n              <p>\n              Existing watermarking algorithms are vulnerable to paraphrase attacks because of their token-level design. To address this issue, we propose SemStamp, a robust sentence-level semantic watermarking algorithm based on locality-sensitive hashing (LSH), which partitions the semantic space of sentences. \n\t      </p>\n            </td>\n          </tr> \n\n\n          <tr bgcolor=\"#ffffd0\">\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/latticegen.jpg\" width=\"220\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/pdf/2309.17157\">\n                <papertitle>LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud</papertitle>\n              </a>\n              <br>\n              Mengke Zhang*, Tianxing He*, Tianle Wang, Lu Mi, Fatemehsadat Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov\n              <br>\n\t      <em>NAACL 2024 Findings</em>\n              <br>\n              <p></p>\n              <p>\n              In the current user-server interaction paradigm for prompted generation, there is zero option for users who want to keep the generated text to themselves. We propose LatticeGen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. In the end, the server does not know what exactly is generated. The key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice.\n\t      </p>\n            </td>\n          </tr> \n\n         <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/2024_kss.jpg\" width=\"220\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2402.11399\">\n                <papertitle>k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text</papertitle>\n              </a>\n              <br>\n              Abe Bohan Hou, Jingyu Zhang, Yichen Wang, Daniel Khashabi, Tianxing He\n              <br>\n\t\t\t\t\t\t\t<em>ACL-Findings 2024</em>\n              <br>\n              <p></p>\n              <p>\n              We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. \n              </p>\n            </td>\n          </tr> \n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/iclr2024_knowledgecard.png\" width=\"200\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2305.09955\">\n                <papertitle>Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models</papertitle>\n              </a>\n              <br>\n              Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov\n              <br>\n\t      <em>ICLR 2024</em>\n              <br>\n              <em>Reviewer Scores: 8/8/8</em>\n              <br>\n              <p></p>\n              <p>\n              We propose Knowledge Card, a community-driven initiative to empower black-box LLMs with modular and collaborative knowledge. By incorporating the outputs of independently trained, small, and specialized LMs, we make LLMs better knowledge models by empowering them with temporal knowledge update, multi-domain knowledge synthesis, and continued improvement through collective efforts.\n              </p>\n            </td>\n          </tr> \n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/emnlp23_detectgen.jpg\" width=\"200\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2310.05165\">\n                <papertitle>On the Zero-Shot Generalization of Machine-Generated Text Detectors</papertitle>\n              </a>\n              <br>\n              Xiao Pu, Jingyu Zhang, Xiaochuang Han, Yulia Tsvetkov, Tianxing He\n              <br>\n\t\t\t\t\t\t\t<em>EMNLP-Findings 2023</em>\n              <br>\n              <p></p>\n              <p>\n              How will the detectors of machine-generated text perform on outputs of a new generator, that the detectors were not trained on? We begin by collecting generation data from a wide range of LLMs, and train neural detectors on data from each generator and test its performance on held-out generators. While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version.\n              </p>\n            </td>\n          </tr> \n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/neurips2023_graph.jpg\" width=\"200\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2305.10037\">\n                <papertitle>Can Language Models Solve Graph Problems in Natural Language?</papertitle>\n              </a>\n              <br>\n              Heng Wang*, Shangbin Feng*, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov\n              <br>\n\t\t\t\t\t\t\t<em>NeurIPS 2023</em>\n              <br>\n              <p></p>\n              <p>\n              Are language models graph reasoners? We propose the NLGraph benchmark, a test bed for graph-based reasoning designed for language models in natural language. We find that LLMs are preliminary graph thinkers while the most advanced graph reasoning tasks remain an open research question.\n              </p>\n            </td>\n          </tr> \n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/neurips2023_neuron.png\" width=\"200\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2311.02258\">\n                <papertitle>Learning Time-Invariant Representations for Individual Neurons from Population Dynamics</papertitle>\n              </a>\n              <br>\n              Lu Mi*, Trung Le*, Tianxing He, Eli Shlizerman, Uygar Sümbül\n              <br>\n\t\t\t\t\t\t\t<em>NeurIPS 2023</em>\n              <br>\n              <p></p>\n              <p>\n              We use implicit dynamical models to learn time-invariant representation for individual neurons from population dynamics and enable mapping functional activity to cell types.\n\t      </p>\n            </td>\n          </tr> \n\n\n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/2022_pcfg_ctg.png\" width=\"200\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2210.07431\">\n                <papertitle>PCFG-based Natural Language Interface Improves Generalization for Controlled Text Generation</papertitle>\n              </a>\n              <br>\n              Jingyu Zhang, James Glass, Tianxing He\n              <br>\n\t      <em>The 2022 Efficient Natural Language and Speech Processing Workshop (NeurIPS ENLSP 2022)</em>\n              <br>\n  \t      <em>The Best Paper Award at the Workshop</em>\n              <br>\n\t      <em>The 12th Joint Conference on Lexical and Computational Semantics (StarSEM 2023)</em>\n              <p></p>\n              <p>\n              We propose a natural language (NL) interface for controlled text generation, where we craft a PCFG to embed the control attributes into natural language commands, and propose variants of existing CTG models that take commands as input.\n              </p>\n            </td>\n          </tr> \n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/acl2022_focus.png\" width=\"200\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2203.01146\">\n                <papertitle>Controlling the Focus of Pretrained Language Generation Models</papertitle>\n              </a>\n              <br>\n              Jiabao Ji, Yoon Kim, James Glass, Tianxing He\n              <br>\n\t\t\t\t\t\t\t<em>ACL-Findings 2022</em>\n              <br>\n              <p></p>\n              <p>\n              Different focus in the context leads to different generation! We develop the \"focus vector\" method to control the focus of a pretrained language model.\n              </p>\n            </td>\n          </tr> \n\n\n          <tr bgcolor=\"#ffffd0\">\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/emnlp2021_exposurebias.png\" width=\"200\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/1905.10617\">\n                <papertitle>Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?</papertitle>\n              </a>\n              <br>\n              Tianxing He, Jingzhao Zhang, Zhiming Zhou, James Glass\n              <br>\n\t\t\t\t\t\t\t<em>EMNLP 2021</em>\n              <br>\n              <p></p>\n              <p>\n                By feeding the LM with different types of prefixes, we could assess how serious exposure bias is. Surprisingly, our experiments reveal that LM has the self-recovery ability, which we hypothesize  to be countering the harmful effects from exposure bias.\n              </p>\n            </td>\n          </tr> \n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/eacl2021_jebm_calibration.png\" width=\"200\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2101.06829\">\n                <papertitle>Joint Energy-based Model Training for Better Calibrated Natural Language Understanding Models</papertitle>\n              </a>\n              <br>\n              Tianxing He, Bryan McCann, Caiming Xiong, Ehsan Hosseini-Asl\n              <br>\n\t\t\t\t\t\t\t<em>EACL 2021</em>\n              <br>\n              <p></p>\n              <p>\n              We explore joint energy-based model (EBM) training during the finetuning of pretrained text encoders (e.g., Roberta) for natural language understanding (NLU) tasks. Our experiments show that EBM training can help the model reach a better calibration that is competitive to strong baselines, with little or no loss in accuracy.\n              </p>\n            </td>\n          </tr> \n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/fewshot_lama.png\" width=\"200\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2109.02772\">\n                <papertitle>An Empirical Study on Few-shot Knowledge Probing for Pretrained Language Models</papertitle>\n              </a>\n              <br>\n              Tianxing He, Kyunghyun Cho, James Glass\n              <br>\n\t\t\t\t\t\t\t<em>On Arxiv</em>\n              <br>\n              <p></p>\n              <p>\n              We compare a variety of approaches under a few-shot knowledge probing setting, where only a small number (e.g., 10 or 20) of example triples are available. In addition, we create a new dataset named TREx-2p, which contains 2-hop relations. \n              </p>\n            </td>\n          </tr> \n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/eacl2021_forgetdialogue.png\" width=\"180\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/1910.07117\">\n                <papertitle>Analyzing the Forgetting Problem in the Pretrain-Finetuning of Dialogue Response Models</papertitle>\n              </a>\n              <br>\n              Tianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing Liu, James Glass, Fuchun Peng\n              <br>\n\t\t\t\t\t\t\t<em>EACL 2021</em>\n              <br>\n              <p></p>\n              <p>\n                After finetuning of pretrained NLG models, does the model forget some precious skills learned pretraining? We demonstrate the forgetting phenomenon through a set of detailed behavior analysis from the perspectives of knowledge transfer, context sensitivity, and function space projection.\n              </p>\n            </td>\n          </tr> \n\n\n\n           <tr bgcolor=\"#ffffd0\">\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/aacl2020_samplingcompare.png\" width=\"180\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2009.07243\">\n                <papertitle>A Systematic Characterization of Sampling Algorithms for Open-ended Language Generation</papertitle>\n              </a>\n              <br>\n              Moin Nadeem*, Tianxing He* (equal contribution), Kyunghyun Cho, James Glass\n              <br>\n\t\t\t\t\t\t\t<em>AACL 2020</em>\n              <br>\n              <p></p>\n              <p>\n              We identify a few interesting properties that are shared among existing sampling algorithms for NLG. We design experiments to check whether these properties are crucial for the good performance.\n              </p>\n            </td>\n          </tr> \n\n\n           <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/iclr2020_gradientclipping.png\" width=\"180\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/1905.11881\">\n                <papertitle>Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity</papertitle>\n              </a>\n              <br>\n              Jingzhao Zhang, Tianxing He, Suvrit Sra, Ali Jadbabaie\n              <br>\n\t      <em>ICLR 2020</em>\n              <br>\n              <em>Reviewer Scores: 8/8/8</em>\n              <br>\n              <p></p>\n              <p>\n                We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples.\n              </p>\n            </td>\n          </tr> \n\n       \n\n          <tr bgcolor=\"#ffffd0\">\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/acl2020_negativetraining.png\" width=\"200\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/1903.02134\">\n                <papertitle>Negative Training for Neural Dialogue Response Generation</papertitle>\n              </a>\n              <br>\n              Tianxing He, James Glass\n              <br>\n\t      <em>ACL 2020</em>\n              <br>\n              <p></p>\n              <p>\n                Can we \"correct\" some detected bad behaviors of a NLG model? We use negative examples to feed negative training signals to the model.\n              </p>\n            </td>\n          </tr> \n\n           <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/autokg_example.png\" width=\"200\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/2008.08995?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529\">\n                <papertitle>AutoKG: Constructing Virtual Knowledge Graphs from Unstructured Documents for Question Answering</papertitle>\n              </a>\n              <br>\n              Seunghak Yu, Tianxing He, James Glass\n              <br>\n\t\t\t\t\t\t\t<em>Preprint</em>\n              <br>\n              <p></p>\n              <p>\n                We propose a novel framework to automatically construct a KG from unstructured documents that does not require external alignment.\n              </p>\n            </td>\n          </tr> \n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/icassl2020_lmadapt_keli.png\" width=\"180\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"papers/20_lmadapt_keli.pdf\">\n                <papertitle>An Empirical Study of Transformer-based Neural Language Model Adaptation</papertitle>\n              </a>\n              <br>\n              Ke Li, Zhe Liu, Tianxing He, Hongzhao Huang, Fuchun Peng, Daniel Povey, Sanjeev Khudanpur\n              <br>\n\t\t\t\t\t\t\t<em>ICASSP 2020</em>\n              <br>\n              <p></p>\n              <p>\n                We propose a mixer of dynamically weighted LMs that are separately trained on source and target domains, aiming to improve simple linear interpolation with dynamic weighting.\n              </p>\n            </td>\n          </tr> \n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/iclr2019_detectegregious.png\" width=\"180\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/1809.04113\">\n                <papertitle>Detecting Egregious Responses in Neural Sequence-to-sequence Models</papertitle>\n              </a>\n              <br>\n              Tianxing He, James Glass\n              <br>\n\t\t\t\t\t\t\t<em>ICLR 2019</em>\n              <br>\n              <p></p>\n              <p>\n                Can we trick dialogue response models to emit dirty words?\n              </p>\n            </td>\n          </tr> \n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/iscslp16_birnnlmnce.png\" width=\"180\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"https://arxiv.org/abs/1602.06064\">\n                <papertitle>On Training Bi-directional Neural Network Language Model with Noise Contrastive Estimation</papertitle>\n              </a>\n              <br>\n              Tianxing He, Yu Zhang, Jasha Droppo, Kai Yu\n              <br>\n\t\t\t\t\t\t\t<em>ISCSLP 2016</em>\n              <br>\n              <p></p>\n              <p>\n                We attempt to train a bi-directional RNNLM via noise contrastive estimation.\n              </p>\n            </td>\n          </tr> \n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/icassp2016_lstmdnn.png\" width=\"140\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"papers/16lstmdnn_icassp16.pdf\">\n                <papertitle>Exploiting LSTM Structure in Deep Neural Networks for Speech Recognition</papertitle>\n              </a>\n              <br>\n              Tianxing He, Jasha Droppo\n              <br>\n\t\t\t\t\t\t\t<em>ICASSP 2016</em>\n              <br>\n              <p></p>\n              <p>\n                We design a LSTM structure in the depth dimension, instead of its original use in time-step dimension.\n              </p>\n            </td>\n          </tr> \n\n\n          <tr>\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/icassp2015_charembed.png\" width=\"180\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"papers/15mrnnlm_icassp15.pdf\">\n                <papertitle>Recurrent Neural Network Language Model with Structured Word Embeddings for Speech Recognition</papertitle>\n              </a>\n              <br>\n              Tianxing He, Xu Xiang, Yanmin Qian, Kai Yu\n              <br>\n\t\t\t\t\t\t\t<em>ICASSP 2015</em>\n              <br>\n              <p></p>\n              <p>\n                We restructure word embeddings in a RNNLM to take advantage of its sub-units.\n              </p>\n            </td>\n          </tr> \n\n\n          <tr bgcolor=\"#ffffd0\">\n            <td style=\"padding:20px;width:25%;vertical-align:middle\">\n                  <img src=\"images/nodeprune_papericon.png\" width=\"180\">\n            </td>\n            <td style=\"padding:20px;width:75%;vertical-align:middle\">\n              <a href=\"papers/14prundnn_icassp14.pdf\">\n                <papertitle>Reshaping Deep Neural Network for Fast Decoding by Node-Pruning</papertitle>\n              </a>\n              <br>\n              Tianxing He, Yuchen Fan, Yanmin Qian, Tian Tan, Kai Yu\n              <br>\n\t\t\t\t\t\t\t<em>ICASSP 2014</em>\n              <br>\n              <p></p>\n              <p>\n                We prune neurons of a DNN for faster inference.\n              </p>\n            </td>\n          </tr> \n\n\n\n\n        </tbody></table>\n\n        <table style=\"width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;\"><tbody>\n          <tr>\n            <td style=\"padding:0px\">\n              <br>\n              <p style=\"text-align:right;font-size:small;\">\n                 The design and code of this website is borrowed from <a href=\"https://github.com/jonbarron/jonbarron_website\">Jon Barron's site</a>.\n              </p>\n            </td>\n          </tr>\n        </tbody></table>\n      </td>\n    </tr>\n  </tbody></table>\n\n\n\n</body></html>"
}